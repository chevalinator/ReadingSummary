# How to Measure Anything Workbook: Finding the Value of Intangibles in Business
![](https://github.com/mathieuchevalier/ReadingSummary/blob/master/Poor%20Charlie%20Almanack%20-%20Charlie%20Munger/img/Charlie.jpg)

The quotes below are taken from [How to Measure Anything (Second Edition)](https://www.amazon.com/Measure-Anything-Second-Douglas-Hubbard/dp/B00A2N3KQ6). 


# Quotes


I wrote this book to correct a costly myth that permeates many organizations today: that certain things can’t be measured.

----


When you can measure what you are speaking about, and express it in numbers, you know something about it; but when you cannot express it in numbers, your knowledge is of a meager and unsatisfactory kind; it may be the beginning of knowledge, but you have scarcely in your thoughts advanced to the state of science. —Lord Kelvin, British physicist and member of the House of Lords, 1824–1907

----


Anything can be measured. If a thing can be observed in any way at all, it lends itself to some type of measurement method. No matter how “fuzzy” the measurement is, it’s still a measurement if it tells you more than you knew before. And those very things most likely to be seen as immeasurable are, virtually always, solved by relatively simple measurement methods.

----


Intangibles that appear to be completely intractable can be measured. 2. This measurement can be done in a way that is economically justified.

----


You may have run into one or more of these real-life examples of so-called intangibles: Management effectiveness The forecasted revenues of a new product The public health impact of a new government environmental policy The productivity of research The “flexibility” to create new products The value of information The risk of bankruptcy The chance of a given political party winning the White House The risk of failure of an information technology (IT) project Quality

----


The fact of the matter is that some organizations have succeeded in analyzing and measuring all of the previously listed items, using methods that are probably less complicated than you would think.

----


The Proposal Let me begin by stating the three propositions as a way to define and approach the problem of measurement in business: 1. Management cares about measurements because measurements inform uncertain decisions. 2. For any decision or set of decisions, there are a large combination of things to measure and ways to measure them—but perfect certainty is rarely a realistic option. 3. Therefore, management needs a method to analyze options for reducing uncertainty about decisions.

----


From here on out, this book addresses three broad issues: why nothing is really immeasurable, how to set up and define any measurement problem, and how to use powerful and practical measurement methods to resolve the problem.

----


The best-known example of such a “Fermi question” was Fermi asking his students to estimate the number of piano tuners in Chicago.

----


Fermi would start by asking them to estimate other things about pianos and piano tuners that, while still uncertain, might seem easier to estimate. These included the current population of Chicago (a little over 3 million in the 1930s to 1950s), the average number of people per household (2 or 3), the share of households with regularly tuned pianos (not more than 1 in 10 but not less than 1 in 30), the required frequency of tuning (perhaps 1 a year, on average), how many pianos a tuner could tune in a day (4 or 5, including travel time), and how many days a year the turner works (say, 250 or so). The result would be computed: Tuners in Chicago = Population/people per household ×percentage of households with tuned pianos ×tunings per year/ (tunings per tuner per day × workdays per year) Depending on which specific values you chose, you would probably get answers in the range of 20 to 200, with something around 50 being fairly common.

----


When this number was compared to the actual number (which Fermi might get from the phone directory or a guild list), it was always closer to the true value than the students would have guessed. This may seem like a very wide range, but consider the improvement this was from the “How could we possibly even guess?” attitude his students often started with.

----


This approach to solving a Fermi question is known as a Fermi decomposition or Fermi solution. This method helped to estimate the uncertain quantity but also gave the estimator a basis for seeing where uncertainty about the quantity came from. Was the big uncertainty about the share of households that had tuned pianos, how often a piano needed to be tuned, how many pianos can a tuner tune in a day, or something else? The biggest source of uncertainty would point toward a measurement that would reduce the uncertainty the most.

----


assessing what you currently know about a quantity is a very important step for measurement of those things that do not seem as if you can measure them at all.

----


Fermi Decomposition for a New Business Chuck McKay, with Wizard of Ads, encourages companies to use Fermi questions to estimate the market size for a product in a given area. An insurance agent once asked Chuck to evaluate an opportunity to open a new office in Wichita Falls, Texas, for an insurance carrier that currently had no local presence there. Is there room for another carrier in this market? To test the feasibility of this business proposition, McKay answered a few Fermi questions with some Internet searches. Like Fermi, McKay started with the big population questions and proceeded from there. According to City-Data.com, there were 62,172 cars in Wichita Falls. According to the Insurance Information Institute, the average automobile insurance annual premium in the state of Texas was $837.40. McKay assumed that almost all cars have insurance, since it is mandatory, so the gross insurance revenue in town was $52,062,833 each year. The agent knew the average commission rate was 12%, so the total commission pool was $6,247,540 per year. According to Switchboard.com, there were 38 insurance agencies in town, a number that is very close to what was reported in Yellowbook.com. When the commission pool is divided by those 38 agencies, the average agency commissions are $164,409 per year. This market was probably getting tight since City-Data.com also showed the population of Wichita Falls fell from 104,197 in 2000 to 99,846 in 2005. Furthermore, a few of the bigger firms probably wrote the majority of the business, so the revenue would be even less than that—and all this before taking out office overhead. McKay’s conclusion: A new insurance agency with a new brand in town didn’t have a good chance of being very profitable, and the agent should pass on the opportunity. (Note: These are all exact numbers. But soon we will discuss how to do the same kind of analysis when all you have are inexact ranges.)

----


Left to chance alone, they should get about 50% right with a 95% confidence interval of +/− 6%. (If you flipped 280 coins, there is a 95% chance that between 44% and 56% would be heads.)

----


James Randi, retired magician and renowned skeptic, set up this foundation for investigating paranormal claims scientifically. (He advised Emily on some issues of experimental protocol.) Randi created the $1 million “Randi Prize” for anyone who can scientifically prove extrasensory perception (ESP), clairvoyance, dowsing, and the like.

----


Emily’s example provides more than one lesson for business. First, even touchy-feely-sounding things like “employee empowerment,” “creativity,” or “strategic alignment” must have observable consequences if they matter at all. I’m not saying that such things are “paranormal,” but the same rules apply.

----


If the therapists can do what they claimed, then they must, Emily reasoned, at least be able to feel the energy field. If they can’t do that (and it is a basic assumption of the claimed benefits), then everything about therapeutic touch is in doubt.

----


MII really improves the quality of deliverables, then it should affect customer perceptions and ultimately revenue.7 Simply ask a random sample of customers to rank the quality of some pre-MII and post-MII deliverables (make sure they don’t know which (continued ) (Continued ) is which) and if improved quality has recently caused them to purchase more services from Mitre.

----


Taken together, Eratosthenes, Enrico, and Emily show us something very different from what we are typically exposed to in business. Executives often say, “We can’t even begin to guess at something like that.” They dwell ad infinitum on the overwhelming uncertainties. Instead of making any attempt at measurement, they prefer to be stunned into inactivity by the apparent difficulty in dealing with these uncertainties. Fermi might say, “Yes, there are a lot of things you don’t know, but what do you know?”

----


Other managers might object: “There is no way to measure that thing without spending millions of dollars.” As a result, they opt not to engage in a smaller study—even though the costs might be very reasonable—because such a study would have more error than a larger one. Yet perhaps even this uncertainty reduction might be worth millions, depending on the size and frequency of the decision it is meant to support.

----


find that the reason intangibles seem intangible is almost never for lack of the most sophisticated measurement methods. Usually things that seem immeasurable in business reveal themselves to much simpler methods of observation, once we learn to see through the illusion of immeasurability. In this context, Fermi’s value to us is in how we determine our current state of knowledge about a thing as a precursor to further measurement.

----


T here are just three reasons why people think that something can’t be measured. Each of these three reasons is actually based on misconceptions about different aspects of measurement. I will call them concept, object, and method. 1. Concept of measurement. The definition of measurement itself is widely misunderstood. If one understands what “measurement” actually means, a lot more things become measurable. 2. Object of measurement. The thing being measured is not well defined. Sloppy and ambiguous language gets in the way of measurement. 3. Methods of measurement. Many procedures of empirical observation are not well known. If people were familiar with some of these basic methods, it would become apparent that many things thought to be immeasurable are not only measurable but may already have been measured.

----


In addition to these reasons why something can’t be measured, there are also three common reasons why something “shouldn’t” be measured. The reasons often given for why something “shouldn’t” be measured are: 1. The economic objection to measurement (i.e., any measurement would be too expensive) 21 2. The general objection to the usefulness and meaningfulness of statistics (i.e., “You can prove anything with statistics.”) 3. The ethical objection (i.e., we shouldn’t measure it because it would be immoral to measure it)

----


the concept of measurement, or rather the misconception of it, is probably the most important obstacle to overcome. If we incorrectly think that measurement means meeting some nearly unachievable standard of certainty, then few things will seem measurable.

----


Definition of Measurement Measurement: A quantitatively expressed reduction of uncertainty based on one or more observations.

----


The practical differences between this definition and the most popular definitions of measurement are enormous. Not only does a true measurement not need to be infinitely precise to be considered a measurement, but the lack of reported error—implying the number is exact—can be an indication that empirical methods, such as sampling and experiments, were not used (i.e., it’s not really a measurement at all). Real scientific methods report numbers in ranges, such as “the average yield of corn farms using this new seed increased between 10% and 18% (95% confidence interval).” Exact numbers reported without error might be calculated “according to accepted procedure,” but, unless they represent a 100% complete count (e.g., the change in my pocket), they are not necessarily based on empirical observation

----


So a measurement doesn’t have to eliminate uncertainty after all. A mere reduction in uncertainty counts as a measurement and possibly can be worth much more than the cost of the measurement. But there is another key concept of measurement that would surprise most people: A measurement doesn’t have to be about a quantity in the way that we normally think of it. Note where the definition I offer for measurement says a measurement is “quantitatively expressed.” The uncertainty, at least, has to be quantified, but the subject of observation might not be a quantity itself—it could be entirely qualitative, such as a membership in a set. For example, we could “measure” whether a patent will be awarded or whether a merger will happen while still satisfying our precise definition of measurement. But our uncertainty about those observations must be expressed quantitatively (e.g., there is an 85% chance we will win the patent dispute; we are 93% certain our public image will improve after the merger, etc.).

----


In my seminars, I often ask the audience to challenge me with difficult or seemingly impossible measurements. In one case, a participant offered “mentorship” as something difficult to measure. I said, “That sounds like something one would like to measure. I might say that more mentorship is better than less mentorship. I can see people investing in ways to improve it, so I can understand why someone might want to measure it. So, what do you mean by ‘mentorship’?” The person almost immediately responded, “I don’t think I know,” to which I said, “Well, then maybe that’s why you believe it is hard to measure. You haven’t figured out what it is.”

----


In 2000, when the Department of Veterans Affairs asked me to help define performance metrics for IT security, I asked: “What do you mean by ‘IT security’?” and over the course of two or three workshops, the department staff defined it for me. They eventually revealed that what they meant by “IT security” were things like a reduction in unauthorized intrusions and virus attacks. They proceeded to explain that these things impact the organization through fraud losses, lost productivity, or even potential legal liabilities (which they may have narrowly averted when they recovered a stolen notebook computer in 2006 that contained the Social Security numbers of 26.5 million veterans). All of the identified impacts were, in almost every case, obviously measurable.

----


The clarification chain is just a short series of connections that should bring us from thinking of something as an intangible to thinking of it as a tangible. First, we recognize that if X is something that we care about, then X, by definition, must be detectable in some way. How could we care about things like “quality,” “risk,” “security,” or “public image” if these things were totally undetectable, in any way, directly or indirectly? If we have reason to care about some unknown quantity, it is because we think it corresponds to desirable or undesirable results in some way. Second, if this thing is detectable, then it must be detectable in some amount. If you can observe a thing at all, you can observe more of it or less of it. Once we accept that much, the final step is perhaps the easiest. If we can observe it in some amount, then it must be measurable.

----


Clarification Chain 1. If it matters at all, it is detectable/observable. 2. If it is detectable, it can be detected as an amount (or range of possible amounts). 3. If it can be detected as a range of possible amounts, it can be measured.

----


If the clarification chain doesn’t work, I might try a “thought experiment.” Imagine you are an alien scientist who can clone not just sheep or even people but entire organizations. Let’s say you were investigating a particular fast food chain and studying the effect of a particular intangible, say, “employee empowerment.” You create a pair of the same organization calling one the “test” group and one the “control” group. Now imagine that you give the test group a little bit more “employee empowerment” while holding the amount in the control group constant. What do you imagine you would actually observe—in any way, directly or indirectly—that would change for the first organization? Would you expect decisions to be made at a lower level in the organization? Would this mean those decisions are better or faster? Does it mean that employees require less supervision? Does that mean you can have a “flatter” organization with less management overhead? If you can identify even a single observation that would be different between the two cloned organizations, then you are well on the way to identifying how you would measure it.

----


It is also imperative to state why we want to measure something in order to understand what is really being measured. The purpose of the measurement is often the key to defining what the measurement is really supposed to be. In the first chapter, I argued that all measurements of any interest to a manager must support at least one specific decision. For example, I might be asked to help someone measure the value of crime reduction. But when I ask why they care about measuring that, I might find that what they really are interested in is building a business case for a specific biometric identification system for criminals. Or I might be asked how to measure collaboration only to find that the purpose of such a measurement would be to resolve whether a new document management system is required.

----


Business managers need to realize that some things seem intangible only because they just haven’t defined what they are talking about. Figure out what you mean and you are halfway to measuring it.

----


Measuring with very small random samples: You can learn something from a small sample of potential customers, employees, and so on especially when there is currently a great deal of uncertainty. Measuring the population of things that you will never see all of: There are clever and simple methods for measuring the number of a certain type of fish in the ocean, the number of plant species in the rain forests, the number of production errors in a new product, or the number of unauthorized access attempts in your system that go undetected.

----


Measuring when many other, even unknown, variables are involved: We can determine whether the new “quality program” is the reason for the increase in sales as opposed to the economy, competitor mistakes, or a new pricing policy.

----


Measuring the risk of rare events: The chance of a launch failure of a rocket that has never flown before, or another September 11 attack, another levee failure in New Orleans, or another major financial crisis can all be informed in valuable ways through observation and reason.

----


Measuring subjective preferences and values: We can measure the value of art, free time, or reducing risk to your life by assessing how much people actually pay for these things.

----


Here is a very simple example of a quick measurement anyone can do with an easily computed statistical uncertainty. Suppose you want to consider more telecommuting for your business. One relevant factor when considering this type of initiative is how much time the average employee spends commuting every day. You could engage in a formal office-wide census of this question, but it would be time consuming and expensive and will probably give you more precision than you need. Suppose, instead, you just randomly pick five people. There are some other issues we’ll get into later about what constitutes “random,” but, for now, let’s just say you cover your eyes and pick names from the employee directory. Call these people and, if they answer, ask them how long their commute typically is. When you get answers from five people, stop. Let’s suppose the values you get are 30, 60, 45, 80, and 60 minutes. Take the highest and lowest values in the sample of five: 30 and 80. There is a 93.75% chance that the median of the entire population of employees is between those two numbers. I call this the “Rule of Five.” The Rule of Five is simple, it works, and it can be proven to be statistically valid for a wide range of problems. With a sample this small, the range might be very wide, but if it was significantly narrower than your previous range, then it counts as a measurement.

----


Rule of Five There is a 93.75% chance that the median of a population is between the smallest and largest values in any random sample of five from that population.

----


It might seem impossible to be 93.75% certain about anything based on a random sample of just five, but it works. To understand why this method works, it is important to note that the Rule of Five estimates the median of a population. The median is the point where half the population is above it and half is below it. If we randomly picked five values that were all above the median or all below it, then the median would be outside our range. But what is the chance of that, really? The chance of randomly picking a value above the median is, by definition, 50%—the same as a coin flip resulting in “heads.” The chance of randomly selecting five values that happen to be all above the median is like flipping a coin and getting heads five times in a row. The chance of getting heads five times in a row in a random coin flip is 1 in 32, or 3.125%; the same is true with getting five tails in a row. The chance of not getting all heads or all tails is then 100% – 3.125% × 2, or 93.75%. Therefore, the chance of at least one out of a sample of five being above the median and at least one being below is 93.75% (round it down to 93% or even 90% if you want to be conservative).

----


Many decision makers avoid even trying to make an observation by thinking of a variety of obstacles to measurements. If you want to measure how much time people spend in a particular activity by using a survey, they might say: “Yes, but people won’t remember exactly how much time they spend.” Or if you were getting customer preferences by a survey, they might say: “There is so much variance among our customers that you would need a huge sample.” If you were attempting to show whether a particular initiative increased sales, they respond: “But lots of factors affect sales. You’ll never know how much that initiative affected it.” Objections like this are already presuming what the results of observations will be. The fact is, these people have no idea whether such issues will make measurement futile. They simply presume it. Such critics are working with a set of presumptions about the difficulty of measurement. They might even claim to have a background in measurement that provides some authority (i.e., they took two semesters of statistics 20 years ago). I won’t say those presumptions actually turn out to be true or untrue in every particular case. I will say they are unproductive if they are simply presumptions. What can be inferred from the data already possessed or the likelihood that new data would reduce uncertainty are conclusions that can be made after some specific calculations. But such calculations are virtually never attempted prior to making claims about the impossibility of measurement.

----


Four Useful Measurement Assumptions 1. Your problem is not as unique as you think. 2. You have more data than you think. 3. You need less data than you think. 4. An adequate amount of new data is more accessible than you think.

----


In fact, it is a common misconception that the higher your uncertainty, the more data you need to significantly reduce it. On the contrary, when you know next to nothing, you don’t need much additional data to tell you something you didn’t know before.

----


a proven formula from the field of decision theory allows us to compute a monetary value for a given amount of uncertainty reduction. I put this formula in an Excel macro and, for years, I’ve been computing the economic value of measurements on every variable in dozens of various large business decisions. I found some fascinating patterns through this calculation but, for now, I’ll mention just one: Most of the variables in a business case had an information value of zero. In each business case, something like one to four variables were both uncertain enough and had enough bearing on the outcome of the decision to merit deliberate measurement efforts. Only a Few Things Matter—but They Usually Matter a Lot In business cases, most of the variables have an “information value” at or near zero. But usually at least some variables have an information value that is so high that some deliberate measurement effort is easily justified.

----


what really makes a measurement of high value is a lot of uncertainty combined with a high cost of being wrong.

----


If you can define the outcome you really want, give examples of it, and identify how those consequences are observable, then you can design measurements that will measure the outcomes that matter.

----


Should the death of a very old person be considered equal to that of a younger person, when limited resources force us to make choices? At one point, the EPA considered using what it called a “senior death discount.” A death of a person over 70 was valued about 38% less than a person under 70. Some people were indignant with this and, in 2003, the controversy caused then EPA administrator Christine Todd Whitman to announce that this discount was used for “guidance,” not policy making, and that it was discontinued.

----


Although IQ scores and g surely have various errors and biases, they are, of course, not just mathematical procedures but are based on observations (scores on tests). And since we now understand that measurement does not mean “total lack of error,” the objection that intelligence can’t be measured because tests have error is toothless.

----


Ignorance is never better than knowledge. —Enrico Fermi, winner of the 1938 Nobel Prize for Physics

----


1. Define a decision problem and the relevant uncertainties.Ifpeopleask “How do we measure X?” they may already be putting the cart before the horse. The first question is “What is your dilemma?” Then we can define all of the variables relevant to the dilemma and determine what we really mean by ambiguous ideas like “training quality” or “economic opportunity.” (Chapter 4) 2. Determine what you know now. We need to quantify your uncertainty about unknown quantities in the identified decision. This is done by learning how to describe your uncertainty in terms of ranges and probabilities. (This is a teachable skill.) Defining the relevant decision and how much uncertainty we have about it helps us determine the risk involved. (Chapters 5 and 6) 3. Compute the value of additional information. Information has value because it reduces risk in decisions. Knowing the “information value” of a measurement allows us to identify what to measure as well as inform, us about how to measure it. (Chapter 7) If there are no variables with information values that justify the cost of any measurement approaches, skip to step 5. 4. Apply the relevant measurement instrument(s) to high-value measurements. We will cover some of the basic instruments, such as random sampling, controlled experiments, and some more obscure variations on these. We will also talk about methods that allow us to squeeze more out of limited data, how to isolate the effects of one variable, how to quantify “soft” preferences, how new technologies can be exploited for measurement, and how to make better use of human experts. (Chapters 9 to 13) Repeat step 3. 5. Make a decision and act on it. When the economically justifiable amount of uncertainty has been removed, decision makers face a risk-versusreturn decision. Any remaining uncertainty is part of this choice. To optimize this decision, the risk aversion of the decision maker can be quantified. An optimum choice can be calculated even in situations where there are enormous combinations of possible strategies. We will build on these methods further with a discussion about quantifying risk aversion and other preferences and attitudes of decision makers. This and all of the previous steps are combined into practical project steps. (Chapters 11, 12, and 14)

----


Humans possess a basic instinct to measure, yet this instinct is suppressed in an environment that emphasizes committees and consensus over making basic observations. It simply won’t occur to many managers that an “intangible” can be measured with simple, cleverly designed observations.

----


Prior to making a measurement, we need to answer the following: 1. What is the decision this measurement is supposed to support? 2. What is the definition of the thing being measured in terms of observable consequences? 3. How, exactly, does this thing matter to the decision being asked? 4. How much do you know about it now (i.e., what is your current level of uncertainty)? 5. What is the value of additional information?

----


Uncertainty: The lack of complete certainty, that is, the existence of more than one possibility. The “true” outcome/state/ result/value is not known. Measurement of Uncertainty: A set of probabilities assigned to a set of possibilities. For example: “There is a 60% chance this market will more than double in five years, a 30% chance it will grow at a slower rate, and a 10% chance the market will shrink in the same period.” Risk: A state of uncertainty where some of the possibilities involve a loss, catastrophe, or other undesirable outcome. Measurement of Risk: A set of possibilities each with quantified probabilities and quantified losses. For example: “We believe there is a 40% chance the proposed oil well will be dry with a loss of $12 million in exploratory drilling costs.”

----


When we say that security has improved, we generally mean that particular risks have decreased. If I apply the definition of risk given earlier, a reduction in risk must mean that the probability and/or severity (loss) decreases for a particular list of events. That is the approach I briefly mentioned earlier to help measure one very large IT security investment—the $100 million overhaul of IT security for the Department of Veterans Affairs.

----


They resolved that improved IT security means a reduction in the frequency and severity of a specific list of undesirable events.

----


With these definitions, we have a much more specific understanding of what “improved IT security” really means and, therefore, of how to measure it. When I ask the question “What are you observing when you observe improved IT security?” VA management can now answer specifically. The VA participants realized that when they observe “better security,” they are observing a reduction in the frequency and impact of these detailed events. They achieved the first milestone to measurement.

----


The most important questions of life are indeed, for the most part, really only problems of probability. —Pierre Simon Laplace, Th´eorie Analytique des Probabilit´es, 1812 H

----


In statistics, a range that has a particular chance of containing the correct answer is called a “confidence interval” (CI). A 90% CI is a range that has a 90% chance of containing the correct answer.

----


Overconfidence: When an individual routinely overstates knowledge and is correct less often than he or she expects. For example, when asked to make estimates with a 90% confidence interval, many fewer than 90% of the true answers fall within the estimated ranges. Underconfidence: When an individual routinely understates knowledge and is correct much more often than he or she expects. For example, when asked to make estimates with a 90% confidence interval, many more than 90% of the true answers fall within the estimated ranges.

----


Fortunately, some of the work by other researchers shows that better estimates are attainable when estimators have been trained to remove their Calibrated Estimates: How Much Do You Know Now? 59 personal estimating biases.2 Researchers discovered that odds makers and bookies were generally better at assessing the odds of events than, say, executives. They also made some disturbing discoveries about how bad physicians are at putting odds on unknowns like the chance of a malignant tumor or the chance a chest pain is a heart attack. They reasoned that this variance among different professions shows that putting odds on uncertain things must be a learned skill.

----


Consider one of the 90% CI questions, let’s say the one about when Newton published the Universal Laws of Gravitation. Suppose I offered you a chance to win $1,000 in one of these two ways: 1. You win $1,000 if the true year of publication of Newton’s book turns out to be between the dates you gave for the upper and lower bound. If not, you win nothing. 2. You spin a dial divided into two unequal “pie slices,” one comprising 90% of the dial and the other just 10%. If the dial lands on the large slice, you win $1,000. If it lands on the small slice, you win nothing (i.e., there is a 90% chance you win $1,000). (See Exhibit 5.2.) Which do you prefer? The dial has a stated chance of 90% that you win $1,000, a 10% chance you win nothing. If you are like most people (about 80%), you prefer to spin the dial. But why would that be? The only explanation is that you think the dial has a higher chance of a payoff. The conclusion we have to draw is that the 90% CI you first estimated is really not your 90% CI. It might be your 50%, 65%, or 80% CI, but it can’t be your 90% CI. We say, then, that your initial estimate was probably overconfident. You express your uncertainty in a way that indicates you have less uncertainty than you really have.

----


In my calibration training classes, I’ve been calling this the “equivalent bet test.” (Some examples in the decision psychology literature refer to this as an “equivalent urn” involving drawing random lots from an urn.) As the name implies, it tests to see whether you are really 90% confident in a range by comparing it to a bet which you should consider to be equivalent. Research indicates that even just pretending to bet money significantly improves a person’s ability to assess odds.

----


Some estimators say that when they provide ranges, they think of a single number and then add or subtract an “error” to generate their range. This might seem reasonable, but it actually tends to cause estimators to produce overconfident ranges (i.e., ranges that are too narrow). Looking at each bound alone as a separate binary question of “Are you 95% sure it is over/under this amount?” cures our tendency to anchor.

----


However, it doesn’t appear that any single method completely corrects for the natural overconfidence most people have. To remedy this, I combined several methods and found that most people could be nearly perfectly calibrated.

----


You can also force your natural anchoring tendency to work the other way. Instead of starting with a point estimate and then making it into a range, start with an absurdly wide range and then start eliminating the values you know to be extremely unlikely. If you have no idea how much a new plastic injection molding factory will cost, start with a range of $1,000 to $10 billion and start making it narrower. The new equipment alone will cost $12 million, so you raise the lower bound. A figure of $1 billion is more than all of the other factories you have combined, so you can lower the upper bound. And keep narrowing it from there as you eliminate absurd values.

----


EXHIBIT 5.3 Methods to Improve Your Probability Calibration 1. Repetition and feedback. Take several tests in succession, assessing how well you did after each one and attempting to improve your performance in the next one. 2. Equivalent bets. For each estimate, set up the equivalent bet to test if that range or probability really reflects your uncertainty. 3. Consider two pros and two cons. Think of at least two reasons why you should be confident in your assessment and two reasons you could be wrong. 4. Avoid anchoring. Think of range questions as two separate binary questions of the form “Are you 95% certain that the true value is over/under (pick one) the lower/upper (pick one) bound?” 5. Reverse the anchoring effect. Start with extremely wide ranges and narrow them with the “absurdity test” as you eliminate highly unlikely values.

----


“My 90% confidence can’t have a 90% chance of being right because a subjective 90% confidence will never have the same chance as an objective 90%.”

----


The first statement was made by a chemical engineer and is indicative of the problem he was initially having with calibration. As long as he sees his subjective probability as inferior to objective probability, he won’t get calibrated. However, after a few calibration exercises, he did find that he could subjectively apply odds that were correct as often as the odds implied; in other words, his 90% confidence intervals contained the correct answers 90% of the time.

----


The trivia question was “What is the wingspan of a 747, in feet?” Her answer was 100 to 120 feet. Here is an approximate re-creation of the discussion: Me: Are you 90% sure that the value is between 100 and 120 feet? Calibration Student: I have no idea. It was a pure guess. Me: But when you give me a range of 100 to 120 feet, that indicates you at least believe you have a pretty good idea. That’s a very narrow range for someone who says they have no idea. Calibration Student: Okay. But I’m not very confident in my range. Me: That just means your real 90% confidence interval is probably much wider. Do you think the wingspan could be, say, 20 feet? Calibration Student: No, it couldn’t be that short. Me: Great. Could it be less than 50 feet? Calibration Student: Not very likely. That would be my lower bound. Me: We’re making progress. Could the wingspan be greater than 500 feet? Calibration Student: [pause]...No, it couldn’t be that long. Me: Okay, could it be more than a football field, 300 feet? Calibration Student: [seeing where I was going]...Okay, I think my upper bound would be 250 feet. Me: So then you are 90% certain that the wingspan of a 747 is between 50 feet and 250 feet? Calibration Student: Yes. Me: So your real 90% confidence interval is 50 to 250 feet, not 100 to 120 feet.

----


But if you are allowed to model your uncertainty with ranges and probabilities, you do not have to state something you don’t know for a fact. If you are uncertain, your ranges and assigned probabilities should reflect that. If you have “no idea” that a narrow range is correct, you simply widen it until it reflects what you do know.

----


Me: If your systems are being brought down by a computer virus, how long does the downtime last, on average? As always, all I need is a 90% confidence interval. Security Expert: We would have no way of knowing that. Sometimes we were down for a short period, sometimes a long one. We don’t really track it in detail because the priority is always getting the system back up, not documenting the event. Me: Of course you can’t know it exactly. That’s why we only put a range on it, not an exact number. But what would be the longest downtime you ever had? Security Expert: I don’t know, it varied so much... Me: Were you ever down for more than two entire workdays? Security Expert: No, never two whole days. Me: Ever more than a day? Security Expert: I’m not sure...probably. Me: We are looking for your 90% confidence interval of the average downtime. If you consider all the downtimes you’ve had due to a virus, could the average of all of them have been more than a day? Security Expert: I see what you mean. I would say the average is probably less than a day. Me: So your upper bound for the average would be...? Security Expert: Okay, I think it’s highly unlikely that the average downtime could be greater than 10 hours. Me: Great. Now let’s consider the lower bound. How small could it be? Security Expert: Some events are corrected in a couple of hours. Some take longer. Me: Okay, but do you really think the average of all downtimes could be 2 hours? Security Expert: No, I don’t think the average could be that low. I think the average is at least 6 hours. Me: Good. So is your 90% confidence interval for the average duration of downtime due to a virus attack 6 hours to 10 hours? Security Expert: I took your calibration tests. Let me think. I think there would be a 90% chance if the range was, say, 4 to 12 hours.

----


Throughout this book, I will refer to a 90% CI as a range of values (indicated by an upper and lower bound) that has a 90% probability of containing the true value. I will use this definition regardless of whether the CI was determined subjectively or—as Chapter 9 will show—with sample data. By doing so, I’m using a particular interpretation of probability that treats it as an expression of the uncertainty or “degree of belief” of the person providing the estimate. But many (not all) statistics professors hold a different interpretation that contradicts this. If I computed the 90% CI of, say, the estimate of the mean weight of a new breed of chickens to be 2.45 to 2.78 pounds after three months, they would argue that it is incorrect to say there is a 90% probability that the true population mean is within the interval. They would say the true population mean is either in the range or not.

----


Most decision makers, however, behave as if they take the position I use in this book. They are called “subjectivists,” meaning that they use probabilities to describe a personal state of uncertainty, whether or not it meets criteria like being “purely random.” This position is also sometimes called the “Bayesian” interpretation

----


There is one other extremely important effect of calibration. In addition to improving one’s ability to subjectively assess odds, calibration seems to eliminate objections to probabilistic analysis in decision making. Prior to calibration training, people might feel any subjective estimate was useless. They might believe that the only way to know a CI is to do the math they vaguely remember from first-semester statistics. They may distrust probabilistic analysis in general because all probabilities seem arbitrary to them. But after a person has been calibrated, I have never heard them offer such challenges. Apparently, the hands-on experience of being forced to assign probabilities, and then seeing that this was a measurable skill in which they could see real improvements, addresses these concerns.

----


It is better to be approximately right than to be precisely wrong. —Warren Buffett

----


One of our measurement mentors, Enrico Fermi, was an early user of what was later called a “Monte Carlo simulation.” A Monte Carlo simulation uses a computer to generate a large number of scenarios based on probabilities for inputs. For each scenario, a specific value would be randomly generated for each of the unknown variables. Then these specific values would go into a formula to compute an output for that single scenario. This process usually goes on for thousands of scenarios.

----


If we had all normal distributions and we simply wanted to add or subtract ranges—such as a simple list of costs and benefits—we might not have to run a Monte Carlo simulation. If we just wanted to add up the three types of savings in our example, we can use a simple calculation. Use these six steps to produce a range: 1. Subtract the midpoint from the upper bound for each of the three cost savings ranges: in this example, $20 – $15=$5 for maintenance savings; we also get $5 for labor savings and $3 for materials savings. 2. Square each of the values from the last step: $5 squared is $25, and so on. 3. Add up the results: $25 + $25 + $9 = $59. 4. Take the square root of the total: $59 ˆ.5 = $7.68. 5. Total up the means: $15 + $3 + $6= $24. 6. Add and subtract the result from step 4 from the sum of the means to get the upper and lower bounds of the total, or $24 + $7.68 = $31.68 for the upper bound, $24 – $7.68 = $16.32 for the lower bound.

----


A common simplifying approach to quantifying a risk is simply to multiply the likelihood of some loss times the amount of the loss. This is simple but can be misleading. This assumes the decision maker is “risk neutral.” That is, if I offered you a 10% chance to win $100,000, you would actually be willing to pay as much as $10,000 for it. And you would consider it equivalent to a 50% chance of winning $20,000 or an 80% chance of winning $12,500. But the fact is that most people are not really risk neutral.

----


Determining how much risk is acceptable for a given return is a critical part of an organization’s risk analysis. To make consistent choices, it is important to quantify these various trade-offs in order to clearly state how risk averse or risk tolerant an organization really is. As we will find out later, all sorts of random, arbitrary, and irrelevant factors affect our decisions more than we would like to think. They even affect our preferences more than we would like to think. Documenting what your risk preferences really are is like measuring all risks by the same standard ruler instead of by a ruler that changes every day.

----


Over the years, in case after case, I have found that if organizations apply quantitative risk analysis at all, it is on relatively routine, operationallevel decisions. The largest, most risky decisions are subject to almost no risk analysis—at least not any analysis that an actuary or statistician would be familiar with. I refer to the phenomenon called the “risk paradox.”

----
﻿HowToMeasureAnythingEd2DouglasWHubbard  
- Your Highlight on page 95-95 | Added on Saturday, January 7, 2017 8:02:49 PM

Paradox If an organization uses quantitative risk analysis at all, it is usually for routine operational decisions. The largest, most risky decisions get the least amount of proper risk analysis.

----


EOL is also an expression of risk. It is the simple “risk-neutral” solution first mentioned in Chapter 6. We simply multiply the chance of a loss times the amount of the loss, regardless of how risk averse the decision maker may be. It is a good basis for computing the value of information without getting too complex. But also it is not far off the mark even if we do consider aversion to risk. The cost of measurement is generally small compared to the cost of the decisions the measurement will support. When a risk-averse person takes a large number of very small bets, their choices will be close to risk neutral. With your own money, you may not consider a 20% chance to lose $100,000 to be exactly equal to a certain reward of $20,000, but you may consider a 20% chance of winning $10 to be very close to $2 Likewise, your value of information for each of the potential measurements of a large investment decision would be fairly risk neutral compared to the investment itself.

----


All measurements that have value must reduce the uncertainty of some quantity that affects some decision with economic consequences. The bigger the reduction in EOL, the higher the value of a measurement. The difference between the EOL before a measurement and the EOL after a measurement is called the “Expected Value of Information” (EVI). In other words, the value of information is equal to the value of the reduction in risk.

----


Computing the EVI of a measurement before we make the measurement requires us to estimate how much uncertainty reduction we can expect. This sometimes is complicated, depending on the variable being measured, but there is a shortcut. The easiest measurement value to compute is the Expected Value of Perfect Information (EVPI). If you could eliminate uncertainty, EOL would be reduced to zero. So the EVPI is simply the EOL of your chosen alternative. In the example, the “default” decision (what you would do if you didn’t make a further measurement) was to approve the campaign, and—as explained—that EOL was $2 million. So the value of eliminating any uncertainty about whether this campaign would succeed is simply $2 million. If you could only reduce but not eliminate uncertainty, the EVI would be something less. Value of Information Expected Value of Information (EVI) = Reduction in expected opportunity loss (EOL) or EVI = EOLBefore Info −EOLAfter Info where EOL =chance of being wrong × cost of being wrong Expected Value of Perfect Information (EVPI) = EOLBefore Info (EOL after is zero if information is perfect)

----


One way to compute EVPI for ranges like this is to take these five steps: 1. Slice the distribution up into hundreds or thousands of small segments. 2. Compute the opportunity loss for the midpoint of each segment. 3. Compute the probability for each segment. 4. Multiply the opportunity loss of each segment times its probability. 5. Total all the products from step 4 for all segments.

----


The last example, for the Expected Value of Perfect Information, shows the value of eliminating uncertainty, not just reducing it. The EVPI calculation can be useful by itself, since at least we know a cost ceiling we should never exceed to make the measurement. But often we have to live with merely reducing our uncertainty, especially when we are talking about something like sales forecasts from ad campaigns. At such times it would be helpful to know not just the maximum we might spend under ideal conditions but what a given real-life measurement (with real-life error remaining) should be worth. In other words, we need to know the Expected Value of Information, not the Expected Value of Perfect Information. EVI refers to all information values, whether the information is perfect or not.

----


Once we have eliminated the chance of a loss (or determined for certain that the loss will occur), any additional measurement has no value.

----


Although this method for computing an EVPI with Exhibit 7.4 for normal distributions is just an approximation, it is still very useful. You can estimate the EVI by recognizing EVPI as an absolute ceiling and keeping the general shape of an EVI curve in mind. You may think that we are making approximations upon approximations, but it often results in a “good enough” measurement. Estimating the EVPI for a proposed measurement already has some uncertainty of its own, so fine precision on the EVI is not always that useful. Also, the variables that you should measure—those that have high information values—tend to be the highest information value by an extremely large margin. Often they are 10 or 100 times as much (or more) as the value of the next most valuable measurements. In practice, the estimation error for an EVI usually won’t come close to making a difference in what you select for measurement

----


Additional uncertainty reduction becomes more and more expensive as we approach an uncertainty of zero. In the case of random sampling out of some infinite population, our sample size would have to approach infinity to eliminate uncertainty. However, the uncertainty at first tends to fall away relatively quickly at the beginning of the measurement.

----


someone says a measurement would be too expensive, we have to ask “Compared to what?” If a measurement that would just reduce uncertainty by half costs $50,000 but the EVPI is $500,000, then the measurement certainly is not too expensive. But if the information value is zero, then any measurement is too expensive. Some measurements might have marginal information values—say, a few thousand dollars; not enough to justify some formal effort at measurement but a bit too much just to ignore. For those measurements, I try to think of approaches that can quickly reduce a little uncertainty—say, finding a related study or making a few phone calls to a few more experts.

----


While the EVI curve shows that the value of information levels off, the ECI curve takes off like a rocket as we approach the usually unattainable state of perfect certainty. This fact tells us that we should normally think of measurement as iterative. Don’t try to hit it out of the ballpark in the first attempt. Each measurement iteration can tell you something about how—and whether—to conduct the next iteration.

----


Knowing the shape of the EVI and ECI curves also tells us that a typical assumption about measurement is wrong. It is often assumed that if you have a lot of uncertainty, you need a lot of data to reduce it. In fact, just the opposite is true. When you have a lot of uncertainty, you don’t need much new data to tell you something you didn’t know before. An example from a workshop I once conducted about the measurement of the effectiveness of healthcare issue awareness campaigns illustrates this point. I asked a workshop participant for her 90% confidence interval for the percentage of teens in the Chicago region who have been made aware of the cancer risks of indoor tanning. Her estimate was 2% to 50%. I think the upper bound is very optimistic, but she had a lot of uncertainty and she needed a wide range. With a range this wide, how many teenagers would she have to survey to reduce it significantly? And if her range was only 11% to 15%, how many teenagers would she have to survey to significantly reduce that range? She would have to survey far more people in the second case than in the first to reduce uncertainty significantly. When anyone assumes we need a lot of data to measure something—because it is uncertain—they are invariably making this error.

----


In my consulting practice, I’ve been applying a slightly more sophisticated version of the process I just described. By 1999, I had completed the very quantitative Applied Information Economics analysis on about 20 major investments. At that time, all of my projects still were related only to information technology (IT) investments. Each of these business cases had 40 to 80 variables, such as initial development costs, adoption rate, productivity improvement, revenue growth, and so on. For each of these business cases, I ran a macro in Excel that computed the information value for each variable. I used this value to figure out where to focus measurement efforts. When I ran the macro that computed the value of information for each of these variables, I began to see this pattern: The vast majority of variables had an information value of zero. That is, the current level of uncertainty about that variable was acceptable, and no further measurement was justified (first mentioned in Chapter 3). The variables that had high information values were routinely those that the client never measured. In fact, the high-value variables often were completely absent from previous business cases. (They excluded chance of project cancellation or the risk of low user adoption.) The variables that clients used to spend the most time measuring were usually those with a very low (even zero) information value (i.e., it was highly unlikely that additional measurements of the variable would have any effect on decisions).

----


Low-Value, Typical Measurements Examples: Time spent in an activity Attendance to sales training Near-term costs of a project Number of violations found in safety inspections High-Value, Usually Ignored Measurements Examples: Value of an activity Effect of sales training on sales Long-term benefits of a project Reduction in risk of catastrophic accidents

----


In a business case, the economic value of measuring a variable is usually inversely proportional to how much measurement attention it usually gets.

----


Summarizing Uncertainty, Risk, and Information Value: The First Measurements Understanding how to measure uncertainty is key to measuring risk. Understanding risk in a quantitative sense is key to understanding how to compute the value of information. Understanding the value of information tells us what to measure and about how much effort we should put into measuring it. Putting all of this data in the context of quantifying uncertainty reduction is central to understanding what measurement is all about. They are the three measurements we conduct prior to any other measurement. Putting everything from this chapter together, we can come away with a few new ideas. First, we know that the early part of any measurement usually is the high-value part. Don’t attempt a massive study to measure something if you have a lot of uncertainty about it now. Measure a little bit, remove some uncertainty, and evaluate what you have learned. Were you surprised? Is further measurement still necessary? Did what you learned in the beginning of the measurement give you some ideas about how to change the method? Iterative measurement gives you the most flexibility and the best bang for the buck. Second, if you aren’t computing the value of a measurement, you are very likely measuring some things that are of little or no value and ignoring some high-value items. In addition, if you aren’t computing the value of the measurement, you probably don’t know how to measure it efficiently. You may even be spending too much or spending too little time measuring something. You might dismiss a high-value measurement as “too expensive” because you could not put the cost in context with the value. Lessons from Computing the Value of Information Value of Measurement Matters. If you don’t compute the value of measurements, you are probably measuring the wrong things, the wrong way. Be Iterative. The highest-value measurement is the beginning of the measurement, so do it in bits and take stock after each iteration. Everything up to this point in the book is just “Phase 1” for measuring those things often thought to be impossible to measure.

----


In this chapter, we will ask a few questions so that we might be able to determine the appropriate category of measurement methods. Those questions are: What are the parts of the thing we’re uncertain about? Decompose the uncertain thing so that it is computed from other uncertain things. How has this (or its decomposed parts) been measured by others? Chances are, you’re not the first to encounter a particular measurement problem, and there may even be extensive research on the topic already. Reviewing the work of others is called “secondary research.” How do the “observables” identified lend themselves to measurement? You’ve already answered how you observe the thing. Follow through with that to identify how you observe the parts you identified in the first item above. And secondary research may already answer this for you.

----


How much do we really need to measure it? Take into account the previously computed current state of uncertainty, the threshold, and value of information. These are all clues that point toward the right measurement approach. What are the sources of error? Think about how observations might be misleading. What instrument do we select? Based on your answers to the previous questions, identify and design a measurement instrument. Once again, secondary research may provide guidance.

----


Part of the solution for this initial lack of imagination about measurement instruments may be to try to recapture the fascination Galileo and Fahrenheit had for observing the “secrets” of their environment. They didn’t think of devices for measurement as complex contraptions to be used by esoteric specialists in arcane research. The devices were simple and obvious. Nor were they, like some managers today, dismissive of instruments because they had limitations and errors of their own. Of course they have errors. The question is “Compared to what?” Compared to the unaided human? Compared to no attempt at measurement at all? Keep the purpose of measurement in mind: uncertainty reduction, not necessarily uncertainty elimination.

----


Instruments generally have six advantages. They don’t need to have all the advantages to qualify as instruments; any combination will suffice. Often even one advantage is an improvement on unaided human observation. 1. Instruments detect what you can’t detect.

----


2. Instruments are more consistent.

----


3. Instruments can be calibrated to account for error.

----


4. Instruments deliberately don’t see some things.

----


5. Instruments record.

----


6. Instruments take a measurement faster and cheaper than a human.

----


In our resolve to measure anything, the “Four Useful Measurement Assumptions” (mentioned in Chapter 3) are worth reiterating: 1. It’s been done before—don’t reinvent the wheel. 2. You have access to more data than you think—it might just involve some resourcefulness and original observations. 3. You need less data than you think, if you are clever about how to analyze it. 4. Additional data are probably more accessible than you first thought.

----


One example of the usefulness of decomposition is estimating the cost of a big construction project. Your first calibrated estimate might be $10 million to $20 million based on similar-size projects. However, when you break your specific project down into several components and put a range on each of those, you can end up with an aggregate range that is narrower than your original range. You didn’t make any new observations. You simply made a more detailed model based on things you already knew. Furthermore, you may find that your big uncertainty is the cost of one particular item (e.g., the cost of labor in a particular specialty). This realization alone brings you that much closer to a useful measurement.

----


Decomposition effect: The phenomenon that the decomposition itself often turns out to provide such a sufficient reduction in uncertainty that further observations are not required.

----


Measuring how many people work in the area seems like an obvious and simple step in measurement. Yet those who insist that something cannot ever be measured resist even this. In such cases, a facilitator can be a big help. A facilitated discussion could go like this: Facilitator: Previously you gave me a calibrated estimate of a 5% to 40% productivity improvement for your engineers with this new engineering document management software. Because this particular variable had the highest information value for the business case of whether to invest in the new software, we have to reduce our uncertainty further. Engineer: That’s a problem. How can we measure a soft thing like productivity? We don’t even track document management as an activity, so we have no idea how much time we spend in it now. Facilitator: Well, clearly you think that productivity will improve because there are certain tasks they will spend less time doing, right? Engineer: I suppose so, yes. Facilitator: What activities do engineers spend a lot of time at now that they will spend much less time at if they used this tool? Be as specific as possible. Engineer: Okay. I guess they would probably spend less time searching for relevant documents. But that’s just one item. Facilitator: Great, it’s a start. How much time do they spend at this now per week, and how much do you think that time will be reduced? Calibrated estimates will do for now. Engineer: I’m not sure...I suppose I would be 90% confident the average engineer spends between 1 hour and 6 hours each week just looking for documents. Equipment specs, engineering drawings, procedural manuals, and so on are all kept in different places, and most are not in electronic form. Facilitator: Good. How much of that would go away if they could sit at their desks and do queries? Engineer: Well, even when I use automated search tools like Google, I still spend a lot of time searching through irrelevant data, so automation could not reduce time spent in searching by 100%. But I’m sure it would go down at least by half. Facilitator: Does this vary for the type of engineer? Engineer: Sure. Engineers with management roles spend less time at this. They depend on subordinates more often. However, engineers who focus on particular compliance issues have to research lots of documents. Various technicians also would use this. Facilitator: Okay. How many engineers and technicians fall into each of these categories, and how much time do they each spend in this activity?

----


The 60 or more major risk/return analyses I’ve done in the past 16 years consisted of a total of over 4,000 individual variables, or an average of a little over 60 variables per model. Of those, a little over 120 (about 2 per model) required further measurement according to the information value calculation. Most of these, about 100, had to be decomposed further to find a more easily measured component of the uncertain variable. Other variables offered more direct and obvious methods of measurement, for example, having to determine the gas mileage of a truck on a gravel road (by just driving a truck with a fuel-flow meter) or estimating the number of bugs in software (by inspecting samples of code). But almost a third of the variables that were decomposed (about 30) required no further measurement after decomposition. In other words, about 25% of the 120 high-value measurements were addressed with decomposition alone. Calibrated experts already knew enough about the variable; they just needed a more detailed model that more explicitly expressed the detailed knowledge they had.

----


The standard approach to measurement in business, it seems, is for some smart people to start with the assumption that, being smart, they themselves will have to invent the method for a new measurement. In reality, however, such innovation is almost never required.

----


There are just a few tricks in using the Internet for secondary research. If you are looking for information that has been applied to measurement methods, you will probably find that most Internet searching is unproductive unless you are using the right search terms. It takes practice to use Internet searches effectively, but these tips should help. If I’m really new to a topic, I don’t start with Google. I start with Wikipedia.org,

----


Use search terms that tend to be associated with research and quantitative data. If you need to measure “software quality” or “customer perception,” don’t just search on those terms alone—you will get mostly fluff. Instead, include terms like “table,” “survey,” “control group,” “correlation,” and “standard deviation,” which would tend to appear in more substantive research. Also, terms like “university,” “PhD,” and “national study” tend to appear in more serious (less fluffy) research.

----


Think of Internet research in two levels: search engines and topic-specific repositories. The problem with using powerful search engines like Google is that you might get thousands of hits, none of which is relevant. But try searching specifically within industry magazine Web sites or online academic journals. If I’m curious about macroeconomic or international analysis, I’ll go straight to government Web sites like the Census, Department of Commerce, even the Central Intelligence Agency. (The CIA World Fact Book is my go-to place for a variety international statistical data.) These will give fewer but mostly likely more relevant hits.

----


If you find marginally related research that still doesn’t directly address your topic of interest, be sure to read the bibliography. The bibliography is sometimes the best method for branching out to find more research.

----


3. If it doesn’t appear to leave behind a detectable trail of any kind, and direct observation does not seem feasible without some additional aid, can you devise a way to begin to track it now? If it hasn’t been leaving a trail, you can “tag” it so it at least begins to leave a trail. One example is how Amazon.com provides free gift wrapping in order to help track which books are purchased as gifts. At one point Amazon was not tracking the number of items sold as gifts; the company added the gift-wrapping feature to be able to track it. Another example is how consumers are given coupons so retailers can see, among other things, what newspapers their customers read.

----


4. If tracking the existing conditions does not suffice (with either existing or newly collected data), can the phenomenon be “forced” to occur under conditions that allow easier observation (i.e., an experiment)? Example: If a retail store wants to measure whether a proposed returned-items policy will detrimentally affect customer satisfaction and sales, try it in some stores while holding others unchanged. Try to identify the difference.

----


1. Does it leave a trail of any kind? Just about every imaginable phenomenon leaves some evidence that it occurred. Think like a forensic investigator. Does the thing, event, or activity that you are trying to measure lead to consequences that themselves have a trail of any kind? Example: Longer waits on customer support lines cause some customers to hang up. This has to cause at least some loss of business, but how much? Did they hang up because of some unrelated reason on their end or out of frustration from waiting? People in the first group tend to call back; people in the second group tend not to. If you can identify even some of the customers who hang up and notice that they tend to purchase less, you have a clue. Now can you find any correlation between customers who hung up after long waits and a decrease in sales to that customer? (See “Example for Leaving a Trail.”) 2. If the trail doesn’t already exist, can you observe it directly or at least a sample of it? Perhaps you haven’t been tracking how many customers in a retail parking lot have out-of-state license plates, but you could look now. And even though staking out the parking lot full time is impractical, you can at least count license plates at some randomly selected times. 3. If it doesn’t appear to leave behind a detectable trail of any kind, and direct observation does not seem feasible without some additional aid, can you devise a way to begin to track it now? If it hasn’t been leaving a trail, you can “tag” it so it at least begins to leave a trail. One example is how Amazon.com provides free gift wrapping in order to help track which books are purchased as gifts. At one point Amazon was not tracking the number of items sold as gifts; the company added the gift-wrapping feature to be able to track it. Another example is how consumers are given coupons so retailers can see, among other things, what newspapers their customers read.

----


Some Basic Methods of Observation ■ Follow its trail like a clever detective. Do forensic analysis of data you already have. ■ Use direct observation. Start looking, counting, and/or sampling if possible. ■ If it hasn’t left any trail so far, add a “tracer” to it so it starts leaving a trail. ■ If you can’t follow a trail at all, create the conditions to observe it (an experiment).

----


These methods apply regardless of whether this is a measurement of something that is occurring now (current sales due to customer referral) or a forecast (the expected improvement in customer referrals due to some new product feature, improvement in customer service, etc.). If it is something that describes a current state, the current state has all the information you need to measure it. If the measurement is actually a forecast, consider what you have observed already that gives you any reason to expect improvement change. If you can’t think of anything you ever observed that causes you to have that expectation, why is your expectation justified at all? And remember that in order to detect a trail, add a tracer/tag, or conduct an experiment, you need to observe only a few in a random sample. Also remember that different elements of your decomposition may have to be measured differently. Don’t worry just yet about all of the problems that each of these approaches could entail. Just identify whichever approach seems the simplest and most feasible for now.

----


As a ballpark estimate, I shoot for spending about 10% of the Expected Value of Perfect Information (EVPI) on a measurement and sometimes even as low as 2%. (This is about the least amount you should consider.) I use this estimate for three reasons. 1. The EVPI is the value of perfect information. Since all empirical methods have some error, we are only shooting for a reduction in uncertainty, not perfect information. So the value of our measurement will probably be much less than the EVPI. 2. Initial measurements often change the value of continued measurement. If the first few observations are surprising, the value of continuing the measurement may drop to zero. This means there is a value in iterative measurement. And since you always have the option of continuing a measurement if you need more precision, there is usually a manageable risk in underestimating the initial measurement effort. 3. The information value curve is usually steepest at the beginning. The first 100 samples reduce uncertainty much more than the second 100. Finally, the initial state of uncertainty tells you a lot about how to measure it. Remember, the more uncertainty you started out with, the more the initial observations will tell you. When starting from a position of extremely high uncertainty, even methods with a lot of inherent error can give you more information than you had before.

----


Consider the Error All measurements have error. As with all problems, the solution starts with the recognition that we have the problem—which allows us to develop strategies to compensate, at least partially. Those who tend to be easily thwarted by measurement challenges, however, often assume that the existence of any error means that a measurement is impossible. If that was true, virtually nothing would ever have been measured in any field of science. Fortunately, for the scientific community and for the rest of us, it’s not. Enrico Fermi can rest easy. Scientists, statisticians, economists, and most others who make empirical measurements separate measurement error into two broad types: systemic and random. Systemic errors are those that are consistent and not just random variations from one observation to the next. For example, if the sales staff routinely overestimates next quarter’s revenue by an average of 50%, that is a systemic error. The fact that it isn’t always exactly 50% too optimistic, but varies, is an example of random error. Random error, by definition, can’t be individually predicted but falls into some quantifiable patterns that can be computed with the laws of probability. Systemic error and random error are related to the measurement concepts of precision and accuracy. “Precision” refers to the reproducibility and conformity of measurements, while “accuracy” refers to how close a measurement is to its “true” value. While the terms “accuracy” and “precision” (as well as “inaccuracy” and “imprecision”) are used synonymously by most people, to measurement experts they are clearly different.

----


A bathroom scale that is calibrated to overstate or understate weight (as some people apparently do, deliberately) could be precise but inaccurate. It is precise because if the same person stepped on the scale several times within an hour—so that the actual weight doesn’t have a chance to change—the scale would give the same answer very consistently. Yet it is inaccurate because every answer is always, say, eight pounds over. Now imagine a perfectly calibrated bathroom scale in the bathroom of a moving motor home. Bumps, acceleration, and hills causes the readings on the scale to move about and give different answers even when the same person steps on it twice within one minute. Still, you would find that after a number of times on the scale, the answers average out to be very close to the person’s actual weight. This is an example of fairly good accuracy but low precision. Calibrated experts are similar to the latter. They may be inconsistent in their judgments, but they are not consistently overestimating or underestimating.

----


Quick Glossary of Error Systemic error/bias: An inherent tendency of a measurement process to favor a particular outcome; a consistent bias. Random error: An error that is not predictable for individual observations; not consistent or dependent on known variables (although such errors follow the rules of probability in large groups). Accuracy: A characteristic of a measurement having a low systemic error—that is, not consistently over- or underestimating a value. Precision: A characteristic of a measurement having a low random error; highly consistent results even if they are far from the true value.

----


To put it another way, precision is low random error, regardless of the amount of systemic error. Accuracy is low systemic error, regardless of the amount of random error. Each of the types of error can be accounted for and reduced. If we know the bathroom scale gives an answer eight pounds higher than the true value, we can adjust the reading accordingly. If we get highly inconsistent readings with a well-calibrated scale, we can remove random error by taking several measurements and computing the average. Any method to reduce either of these errors is called a “control.” Random sampling, if used properly, is itself a type of control. Random effects, while individually unpredictable, follow specific predictable patterns in the aggregate. For example, I can’t predict a coin flip. But I can tell you that if you flipped a coin 1,000 times, there will be 500+/−26 heads. (We’ll talk about computing the error range later.) It is often much harder to compute an error range for systemic error. Systemic errors—like those from using biased judges to assess the quality of a work product or using an instrument that constantly underestimates a quantity—don’t necessarily produce random errors that can be quantified probabilistically. If you had to choose, would you prefer the weight measurement from an uncalibrated but precise scale with an unknown error or from a calibrated scale on a moving platform with highly inconsistent readings each time you weigh yourself? I find that, in business, people often choose precision with unknown systemic error over a highly imprecise measurement with random error. For example, to determine how much time sales reps spend in meetings with clients versus other administrative tasks, they might choose a complete review of all time sheets. They would generally not conduct a random sample of sales reps on different days at different times. Time sheets have error, especially those completed for the whole week at 5 P.M. on Friday in a rush to get out the door. People underestimate time spent on some tasks, overestimate time spent on others, and are inconsistent in how they classify tasks.

----


A famous debate about small random versus large nonrandom samples concerned the work of Alfred Kinsey in the 1940s and 1950s regarding sexual behavior. Kinsey’s work was both controversial and popular at the time. Funded by the Rockefeller Foundation, he was able to conduct interviews of 18,000 men and women. But they were not exactly random samples. He tended to meet people by referral and tended to sample everyone in a specific group (a bowling league, a college fraternity, a book club, etc.). Kinsey apparently assumed that any error could be offset by a large enough sample. But that’s not how most systemic error works—it doesn’t “average out.” John W. Tukey, a famous statistician who was retained by the same Rockefeller Foundation to review Kinsey’s work, was quoted as saying: “A random selection of three people would have been better than a group of 300 chosen by Mr. Kinsey.” In another version of this quote, he was said to prefer a random sample of 400 to Kinsey’s 18,000. If the first quote is Tukey’s, he may have exaggerated, but not by much. Tukey meant that the groups Kinsey sampled were often very close to homogeneous. Therefore, these groups may have counted as something closer to one random sample, statistically speaking. In the second version of the quote, Tukey is almost certainly correct:

----


If a complete review of 5,000 time sheets (say, 100 reps for 50 weekly time sheets each) tells us that sales reps spend 34% of their time in direct communication with customers, we don’t know how far from the truth it might be. Still, this “exact” number seems reassuring to many managers. Now, suppose a sample of direct observations of randomly chosen sales reps at random points in time finds that sales reps were in client meetings or on client phone calls only 13 out of 100 of those instances. (We can compute this without interrupting a meeting by asking as soon as the rep is available.) As we will see in Chapter 9, in the latter case, we can statistically compute a 90% CI to be 7.5% to 18.5%. Even though the random sampling approach gives us only a range, we should prefer its findings to the census audit of time sheets. The census of time sheets gives us an exact number, but we have no way to know by how much and in which direction the time sheets err.

----


The error you can’t count on averaging out—systemic error—is also called a “bias.” The list of types of biases seems to grow with almost every year of research in decision psychology or empirical sciences in general. But there are three big biases that you need to control for: expectancy, selection, and observer biases.

----


Few Types of Observation Biases Expectancy bias: Seeing what we want to see. Observers and subjects sometimes, consciously or not, see what they want. We are gullible and tend to be self-deluding. Clinical trials of new drugs have to make sure that subjects don’t actually know whether they have taken the real drug or a placebo. This is the previously mentioned blind test. When those who are taking the real drug are hidden from the doctors as well as the patients, this is a double-blind test. The approach I recommended for the Mitre Corporation example in Chapter 2 is an example of a blind test. Selection bias: Even when attempting randomness in samples, we can get inadvertent nonrandomness. If we sample 500 voters for a poll and 55% say they will vote for candidate A, it is fairly likely—98.8%, to be exact—that candidate A actually has the lead in the population. There is only a 1.2% chance that a random sampling could have just by chance chosen more voters (continued ) (Continued ) for A if A wasn’t actually in the lead. But this assumes the sample was random and didn’t tend to select some types of voters over others. If the sample is taken by asking passersby on a particular street corner in the financial district, you are more likely to get a particular type of voter even if you “randomly” pick which passersby to ask. Observer bias (or the Heisenberg and Hawthorne bias): Subatomic particles and humans have something in common. The act of observing them causes them both to change behavior. In 1927, the physicist Werner Heisenberg derived a formula showing that there is a limit to how much we can know about a particle’s position and velocity. When we observe particles, we have to interact with them (e.g., bounce light off them), causing their paths to change. That same year a research project was begun at the Hawthorne Plant of the Western Electric Company in Illinois. Initially led by Professor Elton Mayo from the Harvard Business School, the study set out to determine the effects of the physical environment and working conditions on worker productivity. Researchers altered lighting levels, humidity, work hours, and so on in an effort to determine under which conditions workers worked best. To their surprise, they found that worker productivity improved no matter how they changed the workplace. The workers were simply responding to the knowledge of being observed; or perhaps, researchers hypothesized, management taking interest in them caused a positive reaction. Either way, we can no longer assume observations see the “real” world if we don’t take care to compensate for how observations affect what we observe. The simplest solution is to keep observations a secret from those being observed.

----


Let’s summarize how to identify the instrument. 1. Decompose the measurement so that it can be estimated from other measurements. Some of these elements may be easier to measure, and sometimes the decomposition itself will have reduced uncertainty. 2. Consider your findings from secondary research. Look at how others measured similar issues. Even if their specific findings don’t relate to your measurement problem, is there anything you can salvage from the methods they used? 3. Place one or more of the elements from the decomposition in one or more of the methods of observation: trails left behind, direct observation, tracking with “tags,” or experiments. Think of at least three ways you detect it, and then follow its trail forensically. If you can’t do that, try a direct observation. If you can’t do that, tag it or make other changes to it so it starts leaving a trail you can follow. If you can’t do that, create the event specifically to be observed (the experiment). 4. Keep the concept of “just enough” squarely in mind. You don’t need great precision if all you need is more certainty that a productivity improvement will be over the minimum threshold needed to justify a project. Keep the information value in mind; a small value means little effort is justified and a big value means you should think bigger about the measurement method. Also, remember how much uncertainty you had to begin with. If you were originally very uncertain, how much of an observation do you really need to reduce the uncertainty? 5. Think about the errors specific to that problem. If it is a series of human judges evaluating the quality of work, beware of expectation bias and consider a blind. If you need a sample, make sure it is random. If your observations themselves can affect outcome, find a way to hide the observation from the subject. Now, if you can’t yet fully visualize the instrument, consider these tips, listed in no particular order. Some have been mentioned already, but all are worth reviewing. Work through the consequences. If the value you are seeking is surprisingly high, what should you see? If the value is surprisingly low, what should you see? In the example cited in Chapter 2, young Emily reasoned that if the therapeutic touch specialists could do what they claimed, they should at least be able to detect a human “aura.” For a quality measurement problem, if quality is better, you probably should see fewer complaints from customers. For a sales-related software application, if a new IT system really helps salespeople sell better, why would you see sales go down for those who use it more? Be iterative. Don’t try to eliminate uncertainty in one giant study. Start making a few observations, and recalculate the information value. It might have a bearing on how you continue measurement. Consider multiple approaches. If one type of observation on one of the elements in your decomposition doesn’t seem feasible, focus on another. You have many options. If the first measurement method works, great. But in some cases I’ve measured things three different ways, after the first two were unenlightening. Are you sure you are exploring all the methods available? If you can’t measure one variable in a decomposition, can you measure another? What’s the really simple question that makes the rest of the measurement moot? Again, Emily didn’t try to measure how well therapeutic touch worked, just whether it worked at all. In the Mitre example discussed earlier, I suggested the company determine if clients could detect any change in the quality of research before it tried to measure a value of the expected improvement in quality. Some questions are so basic that it is possible that their answers could make more complicated measurements irrelevant. What is the basic question you need to ask to see if you need to measure any more? Just do it. Don’t let anxiety about what could go wrong with measurement keep you from just starting to make some organized observations. Don’t assume you won’t be surprised by the first few observations and considerably reduce your uncertainty.

----


The Jelly Bean Example Here is a little experiment you can try. What is your 90% confidence interval (CI) for the weight, in grams, of the average jelly bean? Remember, we need two numbers—a lower bound and an upper bound—just far apart enough that you are 90% confident that the average weight of a jelly bean, in grams, is between the bounds. Just like every other calibrated probability estimate, you have some idea, regardless of how uncertain you feel about it. A gram, by the way, weighs as much as 1 cubic centimeter of water (imagine a thimble full of water). Write down your range before you go any further. As explained in Chapter 5, be sure to test it with the equivalent bet, consider some pros and cons for why the range is reasonable, and test each bound against anchoring. I have a typical bag of jelly beans—the type you can buy anywhere candy is sold. I took such a bag and began sampling jelly beans. I put several jelly beans one at a time on a digital scale. Now consider the following four questions. Answer each one before you go to the next point. 1. Suppose I told you the weight of the first jelly bean I sampled was 1.4 grams. Does that change your 90% CI? If so, what is your updated 90% CI? Write down your new range before proceeding. 2. Now I reveal that the next sample weighed 1.5 grams. Does that change your 90% CI again? If so, what is your CI now? Write down this new range. 3. Now I give you the results of the next three randomly sampled jelly bean weights, for a total of 5 so far: 1.4, 1.6, and 1.1. Does that change your 90% CI even further? If so, what is your 90% CI now? Again, write down this new range. 4. Finally, I give you the results of the next three randomly sampled weights of jelly beans, for a total of eight samples so far: 1.5, 0.9, 1.7. Again, does that change your 90% CI? If so, what is it now? Write down this final range. Your range usually should have gotten at least a little narrower each time you were given more data. If you had an extremely wide range as a first estimate (before you were told any sampling results), then even the first sample would have significantly narrowed your range. I gave this test to nine calibrated estimators and I got fairly consistent results. The biggest difference among the estimators was how uncertain they were about the initial estimate. The narrowest initial range (before sample information was revealed) 1 to 3 grams for the average jelly bean, and the widest was 0.5 to 50 grams, but most ranges were closer to the narrowest ranges. As the estimators were given additional information, most reduced the width of their range, especially those who started with very wide ranges. The estimator who gave a range of 1 to 3 grams did not reduce the range at all after the first sample. But the person who gave a range of 0.5 to 50 grams reduced the upper bound significantly, resulting in a range of 0.5 to 6 grams. The true average of the population of this bag of jelly beans is close to 1.45 grams per jelly bean. Interestingly, the ranges of the estimators narrowed in on this value fairly quickly as they were given just a few additional samples. Exercises like this help you gain a sense of intuition about samples and ranges. Asking calibrated estimators for subjective estimates without applying what some would call “proper statistics” is actually very useful and even has some interesting advantages over traditional statistics, as we will soon see.

----


The t-statistic is similar in shape to the normal distribution we discussed previously. But for very small samples, the shape of the distribution is much flatter and wider. The 90% CI computed with a student’s t-statistic is much more uncertain (i.e., broader) than a normal distribution would indicate. For sample sizes larger than 30, the shape of the t-distribution is virtually the same as the normal distribution.

----


Here is how we compute a 90% CI, using the first five samples from the jelly bean example: 1. Compute the sample “variance.” As the name indicates, this is a way of quantifying how much samples vary from one another using the following steps—a through c. (This is a concept we’ll refer to more often later.) a. Compute the average of the samples: (1.4+1.4+1.5+1.6+1.1)/5 = 1.4 b. Subtract this average from each of the samples and square the result for each sample: (1.4−1.4)2 = 0, (1.4−1.4)2 = 0, (1.5−1.4)2 = .01, etc. c. Add all the squares and divide by 1 less than the number of samples: (0+0+ .01+ .04+ .09)/(5−1) = .035 2. Divide the sample variance by the number of samples and take the square root of the result. In a spreadsheet we could write “= SQRT(.035/5)” to get .0837. (In statistics texts, this is called the “standard deviation of the estimate of the mean.”) 3. Look up the t-stat in Exhibit 9.1, the simplified t-statistic table, next to the sample size. Next to the number 5 is the t-score 2.13. Note that for very large sample sizes, the t-score gets closer to the z-score (for the normal distribution) of 1.645. 4. Multiply the t-stat by the answer from step 2: 2.13 × .0837 = .178. This is the sample error in grams. EXHIBIT 9.1 Simplified t-Statistic Pick the nearest sample size (or interpolate if you prefer more precision). Sample Size t-Score 2 6.31 3 2.92 4 2.35 5 2.13 6 2.02 8 1.89 12 1.80 16 1.75 28 1.70 Larger samples (z-score) 1.645 5. Add the sample error to the mean to get the upper bound of a 90% CI, and subtract the same sample error from the mean to the lower bound: upper bound = 1.4 + .178 = 1.578, lower bound = 1.4 – .178 = 1.222. We get a 90% CI of 1.22 to 1.58 after just five samples.

----


In the experiment with the calibrated estimators and the jelly beans, the estimators consistently gave wider ranges than what we would get if we used the t-statistic, but often not by much. This means that doing a little more math usually reduces error further than calibrated estimators alone. After eight samples, the most conservative calibrated estimator had a range of 0.5 to 2.4 grams while the most confident estimator gave a range of 1 to 1.7 grams. After the same number of samples, the t-statistic gives a 90% CI of 1.21 to 1.57 grams, about the same as the five sample estimate but considerably narrower than the narrowest range among the estimators.

----


But even though the uncertainty reduction according to the estimators was conservative (not as narrow as it could have been), it was not irrational and was still a significant reduction from the prior state of uncertainty. As we will see in Chapter 10, further studies bear out these findings. In summary, we find: When you have a lot of uncertainty, a few samples greatly reduce it, especially with relatively homogeneous populations. In some cases, calibrated estimators were able to reduce uncertainty even with only one sample—which is impossible with the traditional statistics we just discussed. Calibrated estimators are rational yet conservative. Doing more math reduces uncertainty even further.

----


…beyond about 30 samples, you need to quadruple the –100% sample size to cut error by half.

----


Exhibit 9.2 shows the average of relative reduction in uncertainty as sample sizes increase by showing the 90% CI interval getting narrower with each sample. Individual examples will, of course, depend on the data set, but if you could get the average of all the possible sampling problems

----


Exhibit 9.2 shows that after just a few samples, the 90% CI is still wide, but narrows rapidly with each new sample. Also note that while the 90% CI is much narrower at 30 samples, it wasn’t much narrower than at 20 or even 10 samples. In fact, once you get to 30 samples, you have to quadruple the number of samples (120) if you want the error to go down by half again

----


you’ve seen reports of political polls or have read any research that used some sort of sample, you’ve seen reference to the concept of statistical significance. Statistical significance simply tells us whether we are seeing something real and not just something that happened by chance. How big a sample do we need to get a “statistically significant” result?

----


A caveat should be mentioned when applying the methods discussed so far. Both the t-statistic and the normal z-statistic are types of “parametric” statistics. Parametric statistics are those that have to assume a particular underlying distribution. And while often it is safe to assume that a distribution is normal to start with, it can be far off base. Even though these parametric statistics don’t rely strictly on the “subjective” estimates of calibrated experts, they still start with a fairly arbitrary assumption that might be very wrong. As Exhibit 9.2 shows, there are some populations where the estimate of the mean converges quickly. But, if we sample the income levels of individuals, the power of an earthquake, or the size of asteroids in the asteroid belt, we may find that the 90% CI for the estimate of the mean never gets narrower. Some samples will temporarily narrow the 90% CI, but some “outliers” are so much bigger than the rest of the population that, if they came up in the sample, they would greatly widen the CI again. As we sample, this periodic widening from extreme outliers may happen just often enough to keep the estimate of the mean from ever converging.

----


Exhibit 9.3 shows how some things might converge more slowly than others and methods that might apply in each situation.

----

- Your Note on Location 2460 | Added on Monday, January 9, 2017 4:18:50 PM

see exhibit

----


ask: “How big are the exceptions compared to most?” In the case of samples of water from a tank in a municipal water system, the amount of contaminants in one sample will be extremely close to the amount in the next. In those cases, only one sample is required. In the case of how much time per week your coworkers spend in overhead activities not related to a particular project, outliers are unlikely to throw off the average. (There are only so many hours in the week, after all.) In those cases, parametric methods work well. In the case of earthquakes or revenue of companies, a single outlier can easily throw off the average.

----


Exhibit 9.3 are sometimes “power law” distributions. As mentioned in Chapter 6, the normal distribution is not a good fit for some phenomena, such as stock market fluctuations. But the power law is a very good fit. As odd as this might seem, populations that have power law distributions literally have no definable average. But this kind of distribution still has characteristics that can be measured in relatively few observations. These methods are known as “nonparametric.”

----


EXHIBIT 9.4

----


In fact, the mathless table, since it estimates the median, completely avoids the problem of nonconverging estimates. The population can be distributed in all sorts of irregular ways, like the power law distribution of stock market fluctuations, the “camel-back” age distribution in the United States caused by the Baby Boomers and their children, or a uniform distribution like the spin of a roulette wheel. The mathless table still works for the median in these cases. But if the distribution is also symmetrical, regardless of whether it is uniform, normal, camel-back, or bow-tie shaped, then the mathless table also works for the mean.

----


Biased Sample of Sampling Methods How would your average executive measure the population of fish in a lake? I regularly ask this question of a room full of seminar attendees. Usually someone in the room produces the most extreme answer: drain the lake. The average executive, like the average accountant or even the average midlevel information technology (IT) manager, thinks that “measure” is synonymous with “count.” So when asked to measure the population of fish, they assume they are being asked for an exact count, not just a reduction in uncertainty. With that goal in mind, they would drain the lake and, no doubt, would come up with a very organized procedure where a team picks up each dead fish, throws it in the back of a dump truck, and clicks it off on a handheld counter. Perhaps someone else counts the fish again in the truck and inspects the now-empty lake bed to “audit” the quality of the count. He or she could then report that there were exactly 22,573 fish in the lake; therefore, last year’s restocking effort was successful. Of course, they’re all dead now. If you told marine biologists to measure the fish in the lake, they would not confuse a “count” with a “measure.” Instead, the biologists might employ a method called “catch and recatch.” First, they would catch and tag a sample of fish—let’s say 1,000—and release them back into the lake. Then, after the tagged fish had a chance to disperse among the rest of the population, they would catch another sample of fish. Suppose they caught 1,000 fish again, and this time 50 of those 1,000 fish were tagged. This means that about 5% of the fish in the lake are tagged. Since the marine biologists know they originally tagged 1,000 fish, they conclude that the lake contains about 20,000 fish (5% of 20,000 is 1,000). This type of sampling follows the binomial distribution, but, for large numbers like these, we can approximate it with the normal distribution. The error for this estimate can be computed using a slight variation on the previous error-estimating methods. All we have to do is change how we compute the sample variance; the rest is the same. The sample variance in this case is computed as the share within the group we are trying to measure times the share outside of the group. In other words, we take the share of tagged fish (.05) times the share of fish not tagged (.95), resulting in .0475. Now we follow the rest of the previously defined procedure. We divide the sample variance by the number of samples and take the square root of the total: SQRT(.0475 / 1000) = .007. To get our 90% CI of the share of tagged fish in the lake, we take the share we think are tagged (.05) plus or minus .007 times 1.645 (the 90% CI z-statistic) to get a range of 3.8% to 6.2% of the fish in the lake are tagged. We know we tagged 1,000, so this must mean there are a total of 1000/.062 = 16,256 to 1000/.032 = 25,984 fish in the lake.

----


This method is a particularly powerful example of how sampling reveals something about the unseen. It has been used for estimating such things as how many people the U.S. Census missed, how many species of butterflies are still undiscovered in the Amazon, how many unauthorized intrusions have been made in an IT system, and how many prospective customers you have not yet identified. Just because you will never see all of a group doesn’t mean you can’t measure the size of a group.

----


Basically, the recatch method is merely two independent sampling methods where we compare the overlap between the two samples to estimate the size of the population. If you want to estimate the number of flaws in a building design, use two different groups of quality inspectors. Then compare how many they each caught and how many flaws were caught by both teams. The number of flaws each caught is like the number of fish caught in each of the two net castings in the previous example (1,000 each time), and the number of flaws they both found is like the number of tagged fish in the second net (50).

----


The table in Exhibit 9.5 shows the 90% CI for several small sample sizes. If we sample 20, and only 4 have the characteristic we are looking for—in this case, customers who have visited the store’s Web site—then we go to the column for 20 samples and look up the row for 4 “hits.” We find a range of 9.9% to 38% as our 90% confidence interval for the proportion of customers who have been to the Web site. To save space, I don’t show all of the ranges for hits beyond 10. But recall that as long as we have at least 8 hits and 8 hits less than the total sample size, we can use the normal approximation. Also, if we need to get the range for, say, 26 hits out of 30, we can invert the table by treating hits as misses and vice versa. We just get the range for 4 out of 30 hits, 6.6% to 27%, and subtract those values from 100% to get a range of 63% to 93.4%

----


Spot Sampling Spot sampling is a variation of population proportion sampling. Spot sampling consists of taking random snapshots of people, processes, or things instead of tracking them constantly throughout a period of time. For example, if you wanted to see the share of time that employees spend in a given activity, you randomly sample people through the day to see what they were doing at that moment. If you find that in 12 instances out of 100 random samples, people were on a conference call, you can conclude they spend about 12% of the time on conference calls (90% CI is 8% to 18%). At a particular point in time they are either doing this activity or not, and you are simply asking what share of the time this takes. This example is just big enough that we can also approximate it with a normal distribution, as we did earlier. But if you sampled just 10 employees and found that 2 were involved in the given activity, then we can use Exhibit 9.5 to come up with a 7.9% to 47%. As we should always keep in mind, this might seem like a wide range. But if the prior range based on a calibrated estimate was 5% to 70% and the threshold for some decision was 55%, then we have completed a valuable measurement.

----


Clustered Sampling “Clustered sampling” is defined as taking a random sample of groups, then conducting a census or a more concentrated sampling within the group. For example, if you want to see what share of households has satellite dishes or correctly separates plastics in recycling, it might be cost effective to randomly choose several city blocks, then conduct a complete census of everything in a block. (Zigzagging across town to individually selected households would be time consuming.) In such cases, we can’t really consider the number of elements in the groups (in this case, households) to be the number of random samples. Within a block, households may be very similar, so we can’t really treat the number of households as the size of the “random” sample. When households are highly uniform within a block, it might be necessary to treat the effective number of random samples as the number of blocks, not the number of households.

----


Stratified Samples In “stratified sampling,” different sample methods and/or sizes are used for different groups within a population. This method may make sense when you have some groups within a population that vary widely from each other but are fairly homogeneous inside a group. If you are a fast-food restaurant and you want to sample the demographic of your customers, it might make sense to sample drive-through customers differently from walk-ins. If you run a factory and you need to measure “safety habits,” you might try observing janitors and supervisors for safety procedure violations differently from welders. (Don’t forget the Hawthorne effect. Try using a blind in this case.)

----


Exhibit 9.8 shows how a random sample of serial-numbered items can be used to infer the size of the entire population. Following the directions on the exhibit, consider the example of just eight “captured” items. (This could be a competitor’s products, pages of a

----


Where could this apply in business? “Serial numbers”—that is, a sequential series—show up in a variety of places in the modern world. In this way, competitors offer free intelligence of their production levels just by putting serial numbers on items any retail shopper can see. (To be random, however, this sample of items should include those from several stores.)

----


Suppose you needed to measure the average amount of time spent by employees in meetings that could be conducted remotely with one of the Web meeting tools. This could save staff a lot of travel time and even avoid canceled or postponed meetings due to travel difficulties. To determine whether a meeting can be conducted remotely, you need to consider what is done in the meeting. If a meeting is among staff members who communicate regularly and for a relatively routine topic, but someone has to travel to make the meeting, you probably can conduct it remotely. You start out with your calibrated estimate that the median employee spends 3% to 15% traveling to meetings that could be conducted remotely. You determine that if this percentage is actually over 7%, you should make a significant investment in telemeetings. The Expected Value of Perfect Information calculation shows that it is worth no more than $15,000 to study this. According to our rule of thumb for measurement costs, we might try to spend about $1,500. This means anything like a complete census of meetings is out of the question if you have thousands of employees. Let’s say you sampled 10 employees, and, after a detailed analysis of their travel time and meetings in the last few weeks, you find that only 1 spends less time in these activities than the 7% threshold. Given this information, what is the chance that the median time spent in such activities

----


EXHIBIT 9.9 Threshold Probability Calculator

----


Note that the uncertainty about the threshold can fall much faster than the uncertainty about the quantity in general. It’s possible that after just a few samples you still have a fairly wide range, but, if the threshold is well outside the range, the uncertainty about the threshold can drop to virtually nothing. In other words, the funnel in Exhibit 9.9 gets narrower ever faster—regardless of distribution of the population. Because this estimate isn’t thrown out of whack by extreme outliers, it doesn’t matter if the distribution is a power law distribution or not.

----


So, generally, if the samples strongly confirm prior knowledge (e.g., you get just 1 out of 10 samples below the threshold when you already knew that there is a low chance the median is below the threshold), the uncertainty drops even faster. If the samples contradict prior knowledge, it will take more samples to decrease uncertainty by the same amount. Also, remember that the exhibit gives the chance that the median—not the mean—is below or above a threshold. Of course, you can do some more math and reduce uncertainty even further.

----


If we change a feature on a product and want to determine how much this affects customer satisfaction, we might need an experiment. Customer satisfaction and, consequently, repeat business might change for lots of reasons. But if we want to see if this new feature is cost justified, we need to measure its impact apart from anything else. By comparing customers who have bought products with this new feature to customers who did not, we should be better able to isolate the effects of the new feature alone.

----


Suppose a company wanted to measure the effect of customer relationship training on the quality of customer support. The customer support employees typically take incoming calls from clients who have questions or problems with a new product. It is suspected that the main effect of a positive or negative customer support experience is not so much the future sales to that customer but the positive or negative word-of-mouth advertising the company gets as a result. As always, the company started by assessing its current uncertainty about the effects of training, identified the relevant threshold, and computed the value of information. After considering several possible measurement instruments, managers decided that “quality of customer support” should be measured with a postcall survey of customers. The questions, they reasoned, should not just ask whether the customers were satisfied but how many friends they actually told about a positive experience with customer support. Using previously gathered marketing data, the calibrated managers determined that the new customer relationship training could improve sales by 0% to 12% but that they needed to improve it by only 2% to justify the expense of the training (i.e., 2% is the threshold). They begin conducting this survey before anyone attends training, so they can get a baseline. For each employee, they sample only one customer two weeks after they called in. The key question was “Since your support call, to how many friends or family have you recommended any of our products?” The number of people the customer said they made recommendations to is recorded. Knowing some previous research about the impact of word-of-mouth advertising on sales, the marketing department has determined that one more positive report per customer on average results in a 20% increase in sales. The training is expensive, so at first managers decide to send 30 randomly chosen customer support staffers to the training as a test group. EXHIBIT 9.10 Example for a Customer Support Training Experiment Sample Size Mean Variance Test group (received training) 30 2.433 0.392 Control group (did not receive training) 85 2.094 0.682 Original baseline (before anyone received training) 115 2.087 0.659 Nevertheless, the cost of training this small group is still much less than the computed information value. The control group is the entire set of employees who did not receive training. After the test group receives training, managers continue the survey of customers, but again, they sample only one customer for each employee. For the original baseline, the test group, and the control group, the mean and variance are computed (as shown in the jelly bean example at the beginning of this chapter). Exhibit 9.10 shows the results. The responses from customers seem to indicate that the training did help; could it just be chance? Perhaps the 30 randomly chosen staff members were already, on average, better than the average of the group, or perhaps those 30 people were, by chance, getting less problematic customers. For the test group and control group, we apply these five steps: 1. Divide the sample variance of each group by the number of samples in that group. We get .392/30=.013 for the test group and 0.682/85 = .008 for the control group. 2. Add the results from step 1 for each group together: .013 + 008 = .021. 3. Take the square root of the result from step 2. This gives us the standard deviation of the difference between the test group and the control group. The result in this case would be 0.15. 4. Compute the difference between the means of the two groups being compared: 2.433−2.094 = .339. 5. Compute the chance that the difference between the test group and the control group is greater than zero—that is, that the test group really is better than the control group (and not just a fluke). Use the “normdist” formula in Excel to compute this: = normdist(0,0.339,0.15,1) This Excel formula gives a result of 0.01. This shows us that there is only a 1% chance that the test group is really just as good or worse than the control group; we can be 99% certain that the test group is better than the control. We can compare the control group to the original baseline in the same way. The difference between the control group and the original baseline is just .007. Using the same method that we just used to compare the test and the control groups, we find that there is a 48% chance that the control group is less than the baseline or a 52% chance it is higher. This tells us that the difference between these groups is negligible, and, for all practical purposes, they are no different. We have determined with very high confidence that the training contributes to a real improvement in word of mouth advertising. Since the difference between the test and the control groups is about .4, the marketing department concludes that the improved training would account for about an 8% improvement in sales, easily justifying the cost of training the rest of the staff and ongoing training for all new staff. In retrospect, we probably could have used even fewer samples (using a student’s t-distribution for samples under 30).

----


One of the most common questions I get in seminars is something like “If sales increase due to some new IT system, how do I know it went up because of the IT system?” What surprises me a bit about the frequency of this question is the fact that much of the past few centuries of scientific measurement has focused on isolating the effect of a single variable. I can only conclude that those individuals who asked the question do not understand some of the most basic concepts in scientific measurement. Clearly, the experiment example given earlier in this chapter shows how something that has many possible causes can be traced to a particular cause by comparing a test group to a control group. But using a control and a test group is really just one way to separate out the effect of one single variable from all the noise that exists in any business. We can also consider how well one variable correlates with another. Correlation between two sets of data is expressed as a number between +1and −1. A correlation of 1 means the two variables move in perfect harmony: As one increases, so does the other. A correlation of −1 also indicates two closely related variables, but as one increases, the other decreases in lockstep. A correlation of 0 means they have nothing to do with each other.

----


Having said all this, it is important to state three caveats about regression models. First, correlation does not mean “cause.” The fact that one variable is correlated to another does not necessarily mean that one variable causes the other. If church donations and liquor sales are correlated, it is not because of some collusion between clergy and the liquor industry. It is because both are affected by how well the economy is doing. Generally, you should conclude that one thing causes another only if you have some other good reason besides the correlation itself to suspect a cause-and-effect relationship. In the case of the ratings points and promotion weeks, we did have such reasons. Second, keep in mind that these are simple linear regressions. It’s possible to get even better correlations by using some other function of a variable (e.g., its square, inverse, the product of two variables, etc.) than by using the variable itself. Some readers may want to experiment with that. Finally, in multiple regression models, you should be careful of independent variables being correlated to each other. Ideally, independent variables should be entirely unrelated to each other.

----


The advantage of using Excel’s regression tool over some of Excel’s simpler functions, such as =correl(), is that the regression tool can do

----


multiple regression. That is, it can simultaneously compute the coefficients for several independent variables at once. If we were so inclined, we could create a model that would correlate not only promotion time but also season, category, focus group results, and several other factors to ratings. Each of these additional variables would have a coefficient shown as “X Variable 2,” “X Variable 3,” and so on in the summary output table of the regression tool. Putting all these together, we would get a formula like this: Estimated Ratings = “X Variable 1 Coefficient”×Promotion weeks +“X Variable 2 Coefficient”×Focus Group Results +··· +Intercept

----


We haven’t, for example, talked about experiments using the language of “hypothesis testing” developed by the great statistician R.A. Fischer. In a hypothesis test we determine if a particular claim is true or not depending on whether we have reached a stated level of “significance.” The level of significance is an arbitrarily set value indicating the maximum acceptable chance that the findings could be due to chance alone.

----


A Prior-Knowledge Paradox 1. All conventional statistics assume (a) the observer had no prior information about the range of possible values for the subject of the observation, and (b) the observer does have prior knowledge that the distribution of the population is not one of the “inconvenient” ones. 2. The first above assumption is almost never true in the real world and the second is not true more often than we might think.

----


Simple Bayesian Statistics Bayes’ theorem is simply a relationship of probabilities and “conditional” probabilities. A conditional probability is the chance of something given a EXHIBIT 10.1 Bayes’ Theorem P(A |B) = P(A)×P(B|A)/P(B) where: P(A|B) = Conditional probability of A given B P(A) = Probability of A P(B) = Probability of B P(B|A) = Conditional probability of B given A

----


Suppose we were considering whether to release a new product. Historically, new products make a profit in the first year after release only 30% of the time. A mathematician could write this as P (FYP)= 30%, meaning the probability of a first-year profit is 30%. Often a product is released in a test market first before there is a commitment to full-scale production. Of those times when a product was profitable in the first year, it was also successful in the test market 80% of the time (where by “successful” we might mean that a particular sales threshold is met). A mathematician might write this as P(S|FYP) = 80%, meaning the “conditional” probability that a product had a successful test market (S), given (where “given” is the “|” symbol) that we knew it had a first-year profit, is 80%. But we probably wouldn’t be that interested in the probability that the market test was successful, given that there was a first-year profit. What we really want to know is the probability of a first-year profit, given that the market test was successful. That way, the market test can tell us something useful about whether to proceed with the product. This is what Bayes’ theorem does. In this case, we set up Bayes’ theorem with these inputs: P(FYP|S) is the probability of a first-year profit, given a successful test market—in other words, the “conditional” probability of FYP, given S. P(FYP) is the probability of a first-year profit. P(S) is the probability of a successful test market. P(S|FYP) is the probability of a successful test market, given a product that would be widely accepted enough to be profitable the first year. Let’s say that test markets give a successful result 40% of the time. To compute the probability of a first-year profit, given a successful test market, we set up an equation using the probabilities already given: P(FYP |S) = P(FYP)×P(S|FYP)/P(S) = 30%×80%/40% = 60% If the test market is successful, the chance of a first-year profit is 60%. We can also work out what the chance of a first-year profit would be if the test market was not successful by changing two numbers in this calculation. The probability that a profitable product would be successful in the test market was, as we showed, 80%. So the chance that a profitable product would have had an unsuccessful test market is 20%. We would write this as P(∼S|FYP) = 20% where the “∼” symbol means “not.” Likewise, if the probability of a successful test for all products (profitable or not) is 40%, then the overall chance of a test failure must be P(∼S) = 60%. If we substitute P(∼S|FYP) and P(∼S) for P(S|FYP) and P(S), we get: P(FYP|∼S) = P(FYP)×P(∼S|FYP)/P(∼S) = 30%×20%/60% = 10% That is, the chance of a first-year profit is only 10% if the test market is judged to be a failure. For a spreadsheet example of this calculation, see the Web site at www.howtomeasureanything.com.

----


Instinctive Bayesian Approach 1. Start with your calibrated estimate. 2. Gather additional information (polling, reading other studies, etc.). 3. Update your calibrated estimate subjectively, without doing any additional math. I call this an instinctive Bayesian approach because when people update their prior uncertainties with new information, as you did with the jelly beans, there is evidence to believe that those people update their knowledge in a way that is mostly Bayesian.

----


In 1995, Caltech behavioral psychologists Mahmoud A. El-Gamal and David M. Grether studied how people consider prior information and new information when they assess odds.1 They asked 257 students to guess from which of two bingo-like rolling cages balls were drawn. Each cage had balls marked either N or G, but one cage had more N’s than G’s and the other had an equal number of each. Students were told how many balls of each type were drawn after six draws. The students’ job was to determine which of the two cages the balls were drawn from. For example, if a student saw a sample of 6 balls where 5 were N and only 1 was G, the student might be inclined to think it was probably from the cage with more N’s than G’s. However, prior to each draw of 6 balls, the students were told that the cages themselves were randomly selected with a one-third, one-half, or two-thirds probability. In their answers, the students’ seemed to be intuitively computing the Bayesian answer with a slight tendency to overvalue the new information and undervalue the prior information. In other words, they were not quite ideally Bayesian but were more Bayesian than not. They were not perfectly Bayesian because there was also a tendency to ignore knowledge about prior distributions when given new information. Suppose I told you that in a roomful of criminal lawyers and pediatricians, there are 95 lawyers and 5 pediatricians. I randomly pick one person from the group and give you this information about him or her: The person, Janet, loves children and science. Which is more likely: Janet is a lawyer or Janet is a pediatrician? Most people would say Janet was a pediatrician. But even if only 10% of lawyers liked both science and children, there would still be more science and child-loving lawyers than pediatricians. Therefore, it is still more likely that Janet is a lawyer. Ignoring the prior probabilities when interpreting new information is a common error. But it does appear that there are two defenses against this error. 1. Simply being aware of the impact the prior probability has on the problem helps. But, better yet, try to explicitly estimate each of the probabilities and conditional probabilities and attempt to find a set of values that are consistent (an example of this will be shown, shortly). 2. I also find that calibrated estimators were even better at being Bayesian. The students in the study would have been overconfident on most estimating problems if they were like most people. But a calibrated estimator should still have this basic Bayesian instinct while not being as overconfident.

----


In practice, I use a method I call “Bayesian correction” to make subjective calibrated estimates of conditional probabilities internally consistent. I show calibrated estimators what the Bayesian answers would be for some questions given their answers to other questions. They then change their answers until all their subjective calibrated probabilities are at least consistent with each other. For a Bayesian correction, the conditional probabilities have to add up correctly and the inversion has to work out correctly.

----


Once the issue of failing to consider prior probabilities in considered, humans seem to be mostly logical when incorporating new information into their estimates along with the old information. This fact is extremely useful because a human can consider qualitative information that does not fit in standard statistics.

----


This method might cause anxiety among those who see themselves as sticklers for “objective” measurement, but such anxiety would be unwarranted, for three reasons. 1. I’ve already shown that the subjective estimates of calibrated experts are usually closer to rational than irrational, given the adjustments we discussed. 2. This method applies where first-semester “objective” statistics offer no help whatsoever, and, therefore, the only alternative would be to do nothing. 3. Those same people use this method all the time for personal decisions without realizing it. Say they read an article about the possible softening of the housing market that affected their decision to buy or sell a house; was their decision affected because they ran an extensive simulation against a table of data offered in the article? More likely they didn’t and the article did not offer that detail. Rather, they made a qualitative reassessment about a quantity (e.g., a list price). Some controls could be used to offset some of the more legitimate concerns about this method. As a method that relies on human judgment, this approach is subject to several types of previously discussed bias. Here are some controls you could use in the instinctive Bayesian approach: Use impartial judges if possible . If a department head’s budget will be affected by the outcome of the study, don’t rely on that person to assess new information qualitatively. Use blinds when possible. Sometimes it is possible to provide useful information to judges while keeping them in the dark about what specific problem they are assessing. If a marketing report provides transcripts from a focus group for a new product, references to the product can be deleted and judges can still determine if the reaction is positive or negative. Use separation of duties. This can be useful in conjunction with blinds. Get one set of judges to evaluate qualitative information, and give a synopsis to another judge who may be unaware of the specific product, department, technology project, and so on. The second judge can give the final Bayesian response. Precommit to Bayesian consequences. Get judges to say in advance how certain findings would affect their judgments, and apply Bayesian correction until they are internally consistent. This way, when the actual data are available, only the Bayesian formula is used and no further references to subjective judgment are required.

----


I’m calling this method of updating prior knowledge based on dissimilar but somewhat related examples the “heterogeneous benchmark” method. When people feel they have no idea what a quantity might be, just knowing a context of scale, even for unlike items, can be a huge help. If you need to estimate the size of the market for your product in a new city, it helps to know what the size of the market is in other cities. It even helps just to know the relative sizes of the economies of the different cities.

----


Heterogeneous benchmark: A method where calibrated estimators are given other quantities as benchmarks to estimate an uncertain quantity, even when those quantities seem only remotely related. Example: Estimating the sales of a new product by knowing the sales of other products or similar products by competitors.

----


One intriguing example of the heterogeneous benchmark shows up in information technology (IT) security. In the Department of Veterans Affairs IT security example used in Chapters 4 through 6, I showed how we can model various security risks in a quantitative way using ranges and probabilities. But the IT security industry seems to be a bottomless pit of both curious attitudes about things that can’t be measured and the number and type of “intangibles.” One of these supposedly impossible measurements is the “softer costs” of certain catastrophic events. A person who has a lot of experience with the resistance to measurement in IT security is Peter Tippett, formerly of Cybertrust. He applied his MD and PhD in biochemistry in a way that none of his classmates probably imagined: He wrote the first antivirus software. His innovation later became Norton Antivirus. Since then, Tippett has conducted major quantitative studies involving hundreds of organizations to measure the relative risks of different security threats. With these credentials, you might think that his claim that security can be measured would be accepted at face value. Yet many in the IT security industry seem to have a deeply rooted disposition against the very idea that security is measurable at all. Tippett has a name for what he finds to be a predominant mode of thinking about the problem. He calls it the “Wouldn’t it be horrible if...” approach. In this framework, IT security specialists imagine a particularly catastrophic event occurring. Regardless of its likelihood, it must be avoided at all costs. Tippett observes: “Since every area has a ‘wouldn’t it be horrible if...’ all things need to be done. There is no sense of prioritization.” He recalls a specific example. “A Fortune 20 IT security manager wanted to spend $100M on 35 projects. The CIO [chief information officer] wanted to know which projects are more important. His people said nobody knows.” One particular “wouldn’t it be horrible if ...” that Tippett encounters is brand damage, a marred public image. It is possible, imagines the security expert, that something sensitive—like private medical records from a health maintenance organization or the loss of credit card data—could be breached by hackers and exploited. The security expert further imagines that the public embarrassment would so tarnish the brand name of the firm that it should be avoided, whatever the cost and however likely or unlikely it may be. Since the true cost of brand damage or the probability cannot be measured, so this “expert” insists, protection here is just as important as investments guarding against every other catastrophe—which also need to be funded without question. But Tippett did not accept that the magnitude of the brand damage problem was completely indistinguishable from the magnitude of other problems. He devised a method that paired hypothetical examples of brand damage with real events where losses were known. He asked, for example, how much it hurts to have a company’s e-mail go down for an hour, along with other benchmarks. He also asked how much more or less it hurt (e.g. “about the same,” “half as much,” “10 times as much,” etc). Cybertrust already had some idea of the relative scale of the cost of these events from a larger study of 150 “forensic investigations” of loss of customer data. This study included most of the losses of customer data from MasterCard and Visa. Cybertrust surveyed chief executives as well as the general public about perceptions of brand damage. It also compared the actual losses in stock prices of companies after such events. Through these surveys and comparisons, Tippett was able to confirm that the brand damage due to customer data stolen by hackers was worse than the damage caused by misplacing a backup tape. By making several such comparisons with other benchmarks, it was possible to get an understanding of the difference in scale of the different types of catastrophes. Some amount of brand damage was worse than some things but not as bad as others. Furthermore, the relative level of loss could be taken into account along with the probability of that type of loss to compute “expected” loss. I can’t overstate the prior amount of uncertainty regarding this problem. The organizations weren’t just uncertain about how bad brand damage could be. Until Tippett’s study, they had no idea of the order of magnitude of the problem at all. Now they finally have at least a sense of scale of the problem and can differentiate the value of reducing different security risks. At first, Tippett observed a high degree of skepticism in these results at one client, but he notes: “A year later one person is a bit skeptical and the rest are on board.” Perhaps the holdout still insisted that no observation could have reduced his uncertainty. Again, with examples like brand damage, uncertainty is so high that almost any sense of scale is a reduction in uncertainty—therefore, a measurement. Your organization, of course, will probably not set out to conduct a vast survey of over 100 organizations to conduct a measurement. But it is helpful to realize that such studies already exist. (Some firms sell this research.) Also, applying this method even internally can reduce uncertainty, whether your organization purchases external research or not.

----


Bayesian Inversion for Ranges: An Overview As mentioned earlier, many of the other charts and tables I created for this book were done with a type of Bayesian inversion. For most problems in statistics and measurement, we are asking “What is the chance the truth is X, given what I’ve seen?” But it’s actually easier to answer the question “If the truth was X, what was the chance of seeing what I did?” Bayesian inversion allows us to answer the first question by answering the second. Often the second question is much easier to answer. Suppose we have an automotive parts store and we want to measure how many of our customers will still be around next year, given changes in the local economy and traffic patterns on nearby roads. Based on knowledge that retail in general is tightening in this area, our calibrated estimate for the 90% confidence interval (CI) of proportion of current customers who will still be in the area to shop at our auto parts store again in the next year is 35% to 75%. (For now, we will assume a normal distribution.) We computed that if this value is not at least 73%, we would have to postpone an expansion. We also determine that if it is lower than 50%, our only recourse would be relocation to a higher-traffic area. We computed the value of information (using the EOL for ranges method we discussed in Chapter 7) at well over $500,000, so we definitely want to pursue a measurement. But, of course, we want to minimize the burden on customers with customer surveys. Keeping in mind that we Bayesian result (considering original estimate)Sampling results assuming no prior Original calibratedknowledge estimate (robust Bayesian) 0% 20% 40% 60% 80% 100% Percentage of customers who will shop here again in the next 12 months EXHIBIT 10.4 Customer Retention Example Comparison of Prior Knowledge, Sampling without Prior Knowledge, and Sampling with Prior Knowledge (Bayesian Analysis) want to measure incrementally, we see how much information we can get from sampling just 20 customers. If we sample just 20 and 14 of them said they will still be in the area to be customers a year from now, how can we change this range? Remember, typical, non-Bayesian methods can’t consider prior range in the calculation. Before we get into any details of the calculations, let’s just get a visual for what the results would look like, given our initial estimate and the results of the sampling. Exhibit 10.4 shows three different distributions for estimating the percentage of our current customers who will be around in a year. These appear somewhat similar to but not exactly the same as the normal distribution first introduced in Chapter 6. As before, the “hilltops” in each of these distributions are where the outcomes are most likely and the “tails” are unlikely but still possible outcomes. The total area under each curve must add up to 100%. Here is a little more detail on the meaning of each of the three distributions shown in Exhibit 10.4. The leftmost distribution is based on our initial calibrated estimate before we started taking any samples. This is our prior state of uncertainty reflected in our 90% CI of 35% to 75% for customer retention converted to a normal distribution. The rightmost distribution is what our uncertainty about customer retention would look like if we only had our sample results (14 out of 20 customers will be sticking around) and no prior knowledge at all. It assumes only that the percentage of customers who would be around to make a purchase in the next 12 months is somewhere between 0% and 100%. This is also called a “robust Bayesian” distribution. The middle distribution is the result of the Bayesian analysis that considers both the prior knowledge (our calibrated estimate of 35% to 75%) and the sample results (14 out of 20 customers surveyed said they would still be in the area). Notice that the Bayesian result (in the middle) appears to be sort of an average of the two other distributions: the distribution based on prior knowledge only and the distribution based on the sampling results only. But it is more than that. It is also narrower than either of the other distributions. This is an important feature of this method. Prior knowledge based on calibrated estimates and the results of random sampling tell you a little more (sometimes a lot more) than either piece of information alone. Now let’s look at the impact this Bayesian analysis had on our decision. We previously determined that if customer retention was less than 73%, we should defer some planned expansions, and that if it was less than 50%, we should pull up stakes and move to a better area. With only our initial estimate that 35% to 75% of customers will be here in a year, we would be fairly sure we should defer expansion. And since there is a 34% chance that customer retention could be below the 50% threshold and we might have to move. With the sample data alone, we are fairly sure we are not below the 50% threshold but not so sure we are below the 73% threshold. Only with the Bayesian analysis using both the original estimate and the sample data are we fairly confident that we are below the 73% threshold yet above the 50% threshold. We should not move but we should defer planned expansion investments. (See Exhibit 10.5.) We still have uncertainty about the desired course of action but much less than we would without Bayesian analysis. If we still had a significant information value, we might decide to sample a few more customers and repeat the Bayesian analysis. With each new sampling, our range would be even narrower and the effect of our original calibrated estimate would EXHIBIT 10.5 Summary of Results of the Three Distributions versus Thresholds Source of Distribution Confidence in Deferred Expansion (Retention <73%) Confidence in Changing Location (Retention <50%) Based on initial calibrated estimate (35% to 75%) Basedonsamplealone(14of20 surveyed will stay) Bayesian analysis using both initial estimate and sample data 93% 34% 69% 4.3% 91% 6.5% diminish as the new sample data becomes more extensive. If we were to sample, say, over 200 customers, we would find that our initial estimate changed the result very little. Bayesian analysis matters most when we can gather only a few samples to adjust an initial estimate.

----


The binomial distribution allows us to compute the chance of a certain number of “hits,” given a certain number of “trials,” and given the probability of a single hit in one trial. For example, if you were flipping a coin, you could call getting heads a hit, the number of flips would be the trials, and the chance of a hit is 50%.

----


As the number of samples increase, however, the effect of the initial range diminishes. After getting to 60 samples or more, the answer will begin to get closer to what the parametric population

----


As the number of samples increase, however, the effect of the initial range diminishes. After getting to 60 samples or more, the answer will begin to get closer to what the parametric population proportion method would produce.

----


The Lessons of Bayes Although it may seem cumbersome at first, Bayes’ theorem is one of the most powerful measurement tools at our disposal. It is the way it reframes the measurement question that makes it so useful. Given a particular observation, it may seem more obvious to frame a measurement by asking the question “What can I conclude from this observation?” or, in probabilistic terms, “What is the probability X is true, given my observation?” But Bayes showed us that we could, instead, start with the question, “What is the probability of this observation if X were true?” The second form of the question is useful because the answer is often more straightforward and it leads to the answer to the other question. It also forces us to think about the likelihood of different observations given a particular hypothesis and what that means for interpreting an observation. As we saw in the earlier example, if, hypothetically, we know that only 20% of the population will continue to shop at our store, then we can determine the chance exactly 15 out of 20 would say so (using Excel’s “binomdist” function). Then we can invert the problem with Bayes’ theorem to compute the chance that only 20% of the population will continue to shop there given that 15 out of 20 said so in a random sample. We would find that chance to be very nearly zero—about 1 in 16 million.

----


Taken to its logical conclusion, this tool offers a type of rebuttal to a list of common objections to the possibility of a measurement. Skeptics of a measurement often claim that something is immeasurable because they can imagine all sorts of potential errors in a measurement (whether or not they even attempted the measurement yet) and assume that because errors are possible, the observation has no bearing on the measurement. This is simply a misunderstanding of the methods of measurement—one of the three categories of misunderstandings (mentioned in Chapter 3) that lead some to believe there are such things as “immeasurable.” They implicitly assume, without doing the math, that the frequency and magnitude of possible errors means that the observation cannot reduce uncertainty.

----


But Bayes does not let the measurement skeptic off the hook that easily. When we apply Bayes in detail, we find that the conditions that would make an observation meaningless are not so easy to achieve as long as the observation has something to do with the thing being measured. In fact, if a measurement really has no value, it must be shown that there is only one possible state, one possible observation, or that the probability of any possible observation is completely independent of any possible state of the thing being measured. The mere possibility of mistakes or errors in the measurement would not produce complete independence between the observation and the thing being observed. As long as at least one of the possible observations has different probabilities among different states, then any observation will change the probabilities of the states. As long as an observation might tell you something, it must tell you something.

----


Broadly, there are two ways to observe preferences: what people say and what people do. Stated preferences are those that individuals will say they prefer. Revealed preferences are those that individuals display by their actual behaviors. Either type of preference can significantly reduce uncertainty, but revealed preferences are usually, as you might expect, more revealing.

----


It’s not uncommon for managers to feel that concepts such as “quality,” “image,” or “value” are immeasurable. In some cases, this is because they can’t find what they feel to be “objective” estimates of these quantities. But that is simply a mistake of expectations. All quality assessment problems—public image, brand value, and the like—are about human preferences. In that sense, human preferences are the only source of measurement. If that means such a measurement is subjective, then that is simply the nature of the measurement. It’s not a physical feature of any object. It is only how humans make choices about that thing. Once we accept this class of measurements as measurements of human choices alone, then our only question is how to observe these choices. Observing Opinions, Values, and the Pursuit of Happiness Broadly, there are two ways to observe preferences: what people say and what people do. Stated preferences are those that individuals will say they prefer. Revealed preferences are those that individuals display by their actual behaviors. Either type of preference can significantly reduce uncertainty, but revealed preferences are usually, as you might expect, more revealing.

----


Those who specialize in designing surveys often refer to the survey itself as an instrument. Survey instruments are designed to minimize or control for a class of biases called “response bias,” a problem unique to this type of measurement instrument. Response bias occurs when a survey, intentionally or not, affects respondents’ answers in a way that does not reflect their true attitudes. If the bias is done deliberately, the survey designer is angling for a specific response (e.g., “Do you oppose the criminal negligence of Governor...?”), but surveys can be biased unintentionally. Here are five simple strategies for avoiding response bias: 1. Keep the question precise and short. Wordy questions are more likely to confuse. 2. Avoid loaded terms. A “loaded term” is a word with a positive or negative connotation, which the survey designer may not even be aware of, that affects answers. Asking people if they support the “liberal” policies of a particular politician is an example of a question with a loaded term. (It’s also a good example of a highly imprecise question if it mentions no specific policies.) 3. Avoid leading questions. A “leading question” is worded in such a way that it tells the respondent which particular answer is expected. Example: “Should the underpaid, overworked sanitation workers of Cleveland get pay raises?” Sometimes leading questions are not deliberate. Like loaded terms, the easiest safeguard against unintended leading questions is having a second or third person look the questions over. The use of intentional leading questions leads me to wonder why anyone is even taking the survey. If they know what answer they want, what “uncertainty reduction” are they expecting from a survey? 4. Avoid compound questions. Example: “Do you prefer the seat, steering wheel, and controls of car A or car B?” The respondent doesn’t know which question to answer. Break the question into multiple questions. 5. Reverse questions to avoid response set bias. A “response set bias” is the tendency of respondents to answer questions (i.e., scales) in a particular direction regardless of content. If you have a series of scales that ask for responses ranging from 1 to 5, make sure 5 is not always the “positive” response (or vice versa). You want to encourage respondents to read and respond to each question and not fall into a pattern of just checking every box in one column.

----


people say they would prefer to spend $20 on charity for orphans instead of the movies but, in reality, they’ve been to the movies many times in the past year without giving to an orphanage once, then they’ve revealed a preference different from the one they’ve stated. Two good indicators of revealed preferences are things people tend to value a lot: time and money. If you look at how they spend their time and how they spend their money, you can infer quite a lot about their real preferences.

----


There are some very important cautionary points about the use of these methods. For one, the scale itself frames the question in a way that can EXHIBIT 11.1 Partition Dependence Example: How Much Time Will It Take to Put Out a Fire at Building X? Survey I Survey II A: Less than 1 hour B: 1 to 4 hours C: Over 4 hours A: Less than 1 hour B: 1 to 2 hours C: 2 to 4 hours D: 4 to 8 hours E: Over 8 hours have a huge effect on the answers. This is called “partition dependence.” For example, suppose a survey of firefighters asked how long it would take to put out a fire at various facilities.

----


Actually, you can correlate subjective responses to objective measures, and such analysis is done routinely. Some have even applied it to measuring happiness. (See the “Measuring Happiness” inset.) If you can correlate two things to each other, and then if you can correlate one of them to money, you can express both of them in terms of money. And if that seems too difficult, you can even ask them directly, “What are you willing to pay?”

----


Measuring Happiness Andrew Oswald, professor of economics at the University of Warwick, produced a method for measuring the value of happiness.2 He didn’t exactly ask people directly how much they were willing to pay for happiness. Instead, he asked them how happy they are according to a Likert scale and then asked them to state their income and a number of other life events, such as recent family deaths, marriages, children born, and so on. This allowed Oswald to see the change in happiness that would be due to specific life events. He saw how a recent family death decreased happiness or how a promotion increased it. Furthermore, since he was also correlating the effect of income on happiness, he could compute equivalent-income happiness for other life events. He found that a lasting marriage, for example, makes a person just as happy as earning another $100,000 per year.

----


This is why one way to value most things is to ask people how much they are willing to pay for it or, better yet, to determine how much they have been paying for it by looking at past behaviors. The willingness to pay (WTP) method is usually conducted as a random sample survey where people are asked how much they would pay for certain things—usually things that can’t be valued in any other way. The method has been used to value avoiding the loss of an endangered species, improvements in public health and the environment, among others.

----


modification of WTP is the Value of a Statistical Life (VSL) method. With the VSL, people are not directly asked how much they value life but rather how much they are willing to pay for incremental reduction in the risk of death. People routinely make decisions where they, in effect, make a choice between money and a slight reduction in the chance of an early death. You could have spent more on a slightly safer car. Let’s say it amounts to an extra $5,000 for a 20% reduction in dying from an automobile collision, which has only, say, a 0.5% chance of causing your death anyway (given how much you drive, where you drive, your driving habits, etc.), resulting in an overall reduction of mortal risk of one-tenth of 1%. If you opted against that choice, you were saying the equivalent of “I prefer keeping $5,000 to a 0.1% lower chance of premature death.”

----


And, surprisingly, the initial wide ranges on these highly contested values were good enough. I’ve done several risk/return analyses of federal government projects where one component of the benefit of the proposed investment was decreased public health risk. In every one, we simply used wide ranges gathered from a variety of VSL or WTP studies. After computing the value of information, rarely did that range, as wide as it was, turn out to be what required further measurement.

----


By the way, the range many government agencies used, based on a variety of VSL and WTP studies, was $2 million to $20 million to avoid one premature death randomly chosen from the population. If you think that’s too low, look at how you spend your own money on your own safety. Also look at how you choose to spend money on some luxury in your life—no matter how modest—instead of giving more to AIDS or cancer research. If you really thought each and every human life was worth far, far more than that range, you would already be acting differently. When we examine our own behaviors closely, it’s easy to see that only a hypocrite says “Life is priceless.”

----


One common area where these sorts of internal trade-offs have to be made to evaluate something is the tolerance for risk. No one can compute for you how much risk you or your firm should tolerate, but you can measure it. Like the VSL approach, it is simply a matter of examining a list of trade-offs—real or hypothetical—between more reward or lower risk.

----


But perhaps the most important impact I noticed is that working with executives to document the firm’s investment boundary seemed to make all the executives more accepting of quantitative risk analysis in general. Just as the calibration training seemed to dissipate many imagined problems with using probabilistic analysis, the exercise of quantifying risk aversion this way seemed to dissipate concerns about quantitative risk analysis in executive decision making. The executives felt a sense of ownership in the process, and, when they see a proposed investment plotted against their previously stated boundary, they recognize the meaning and relevance of the finding.

----


The impact this fact has on decisions is that risk-adjusted ROI requirements are considerably higher than the typical “hurdle rates”—required minimum ROIs—sometimes used by IT decision makers and chief financial officers. (Hurdle rates are often in the range of 15% to 30%.) This effect increases rapidly as the sizes of proposed projects increase. The typical IT decision maker in the average development environment should require a return of well over 100% for the largest projects in the IT portfolio.

----


When clients say they need help measuring performance, I always ask, “What do you mean by ‘performance’?” Generally, they provide a list of separate observations they associate with performance, such as “This person gets things done on time” or “She gets lots of positive accolades from our clients.” They may also mention factors such as a low error rate in work or a productivity-related measure, such as “error-free modules completed per month.” In other words, they don’t really have a problem with how to observe performance at all. As one client put it: “I know what to look for, but how do I total all these things? Does someone who gets work done on time with fewer errors get a higher performance rating than someone who gets more positive feedback from clients?”

----


This is not really a problem with measurement, then, but a problem of documenting subjective trade-offs. It is a problem of how to tally up lots of different observations into a total “index” of some kind. This is where we can use utility curves to make such tallies consistent. Using them, we can show how we want to make trade-offs similar to these: Is a programmer who gets 99% of assignments done on time and 95% error free better than one who gets only 92% done on time but with a 99% error-free rate? Is total product quality higher if the defect rate is 15% lower but customer returns are 10% higher? Is “strategic alignment” higher if the profit went up by 10% but the “total quality index” went down by 5%? For each of these examples, we can imagine a chart that shows these trade-offs similar to how we charted trade-off preferences for risk and return. Each point on the same curve is considered equally valuable to every other point on that curve. In the investment boundary example, each point on the curve has the identical value of zero. That is, the risk is just barely acceptable given the return, and the decision maker would be indifferent to the options of acceptance versus rejection of the proposed investment.

----


The utility curve between any two things (e.g., quality and timeliness or risk and return) provides for an interesting way to simplify how we express the value of any point on the chart. Since every point can be moved along its curve without changing its value, all points can be considered equivalent to a position on a single standardized line. In this case, we standardize quality and express the relative value of any point on the chart in terms of qualityadjusted, on-time rate. We collapsed two variables into one by answering the question “A worker with error-free rate X and on-time completion Y is just as good as a 95% error-free rate and —— on-time completion rate.”

----


This method of collapsing two different factors can be done no matter how many attributes there are. If, for example, I created utility curves for factor X versus Y and then I create utility curves for Y versus Z, anyone should be able to infer my utility curve for X versus Z. In this manner, several different factors affecting such topics as job performance, evaluating new office locations, choosing a new product line, or anything else can be collapsed into a single standardized measure.

----


it is sometimes useful to collapse all these different considerations into a certain monetary equivalent (CME). The CME of an investment is the fixed and certain dollar amount that the investor considers just as good as the investment. Suppose, for example, I had to buy you out as a partner in a real estate development firm. I give you the option of buying a vacant lot in the Chicago suburbs for $200,000 to do with as you please or I give you $100,000 cash right now. If you were truly indifferent between these choices, you consider the CME of the vacant lot investment to be $100,000. If you thought buying the lot at that price was a fantastically good deal, your CME for the investment might be, say, $300,000. This means you would consider the option of making this investment—with all its uncertainties and risks—to be just as good as being given $300,000 cash in hand. You might have defined trade-offs for dozens of variables to come to this conclusion, but the result couldn’t be simpler. No matter how complicated the variables and their trade-offs get, you will always prefer a $300,000 CME to $100,000 cash.

----


Very often, such trade-offs between different factors do not have to be purely subjective. Sometimes it makes more sense to reduce them to a profit or shareholder value maximization problem. A clever analyst should be able to set up a statistically valid spreadsheet model that shows how error rates, punctuality, and the like affect profit. These solutions all boil down to an argument that there is only one important preference—such as profit—and that the importance of factors like productivity and quality are entirely related to how they affect profit. If this is the case, there is no need to make subjective trade-offs between things like performance and customer satisfaction, quality and quantity, or brand image and revenue.

----


Anchoring. Anchoring is a cognitive bias that was discussed in Chapter 5 on calibration, but it’s worth going into a little further. It turns out that simply thinking of one number affects the value of a subsequent estimate even on a completely unrelated issue.

----


Halo/horns effect. If people first see one attribute that predisposes them to favor or disfavor one alternative, they are more likely to interpret additional subsequent information in a way that supports their conclusion, regardless of what the additional information is. For example, if you initially have a positive impression of a person, you are likely to interpret additional information about that person in a positive light (the halo effect). Likewise, an initially negative impression has the opposite effect (the horns effect).

----


An experiment conducted by Robert Kaplan of San Diego State University shows how physical attractiveness causes graders to give essay writers better evaluations on their essays.

----


Bandwagon bias. If you need something measured, can you just ask a group of people in a room what they think instead of asking them each separately? Additional errors seem to be introduced with that approach.

----


students were part of the experiment and were secretly instructed to choose A, after which the real test subject would pick an answer. When there was one other person in the room who picked the wrong answer, the next person was only 97% likely to choose the right answer. When there were two or three persons choosing the wrong answer before the test subject answered, only 87% and 67%, respectively, chose the right answer.

----


Unfortunately, unfounded belief in the “expert” is not limited to just the movie industry. It exists in a wide range of industries for a variety of problems where it is assumed that the expert is the best tool available. How, after all, can all that knowledge be bested by an algorithm? In fact, the idea that messy problems are always best solved by human experts has been debunked for several decades. In the 1950s, Paul Meehl (1920–2003), an American psychologist, proposed the (still) heretical notion that expert-based clinical judgments about psychiatric patients might not be as good as simple statistical models. A true skeptic, he collected scores of studies showing such things as historical regression analysis, based on medical records, produced diagnoses and prognoses that matched or beat the judgment of doctors and psychoanalysts. As a developer of the test known as the Minnesota Multiphasic Personality Inventory, Meehl was able to show that his personality tests were better than experts at predicting several behaviors regarding neurological disorders, juvenile delinquency, and addictive behaviors.

----


In a World War II study of predictions of how well Navy recruits would perform in boot camp, models based on high school records and aptitude tests outperformed expert interviewers. Even when the interviewers were given the same data, the predictions of performance were best when the expert opinions were ignored.

----


In predicting college freshman GPAs, a simple linear model of high school rank and aptitude tests outperformed experienced admissions staff. In predicting the recidivism of criminals, criminal records and prison records outperformed criminologists. The academic performance of medical school students was better predicted with simple models based on past academic performance than with interviews with professors.

----


Confidence in experts is due, in part, to what Dawes called “the illusion of learning.” They feel as if their judgments must be getting better with time. Dawes believes that this is due, in part, to inaccurate interpretations of probabilistic feedback. Very few experts actually measure their performance over time, and they tend to summarize their memories with anecdotes. They are right sometimes and wrong sometimes, but the anecdotes they remember tend to be more flattering to them. This is also a cause of the previously mentioned overconfidence and why most managers—at least on the first try—tend to perform poorly on calibration tests.

----


A study of experts in horse racing found that as they were given more data about horses, their confidence in their prediction about the outcomes of races improved. Those who were given some data performed better than those who were given none. But as the amount of data they were given increased, actual performance began to level off and even degrade. However, their confidence in their predictions continued to go up even after the information load was making the predictions worse.5 Another study shows that, up to a point, seeking input from others about a decision may improve decisions, but beyond that point the decisions actually get slightly worse as the expert collaborates with more people. Yet, again, the confidence in the decision continues to increase even after the decisions have not improved.

----


A 1999 study measured the ability of subjects to detect lies in controlled tests. Some subjects received training in lie detection and some did not. The trained subjects were more confident in judgements about detecting lies even though they were worse than untrained subjects at detecting lies.

----


You might think that the head of the Information and Decision Sciences Department at the University of Illinois at Chicago (UIC) would come up with a fairly elaborate quantitative method for just about everything. But when Dr. Arkalgud Ramaprasad needed to measure faculty productivity, his approach was much more basic than you might suspect. “Previously they had the ‘stack of paper’ approach,” says Dr. Ram (as he prefers to be called). “The advisory committee would sit around a table covered with files on the faculty and discuss their performance.” In no particular order, they would discuss the publications, grants awarded, proposals written, professional awards, and the like of each faculty member and rate them on a scale of 1 to 5. Based on this unstructured approach, they were making important determinations on such things as faculty pay raises. Dr. Ram felt the error being introduced into the evaluation process was, at this point, mostly one of inconsistently presented data. Almost any improvement in simply organizing and presenting the data in an orderly format would be a benefit. To improve on this situation, he simply organized all the relevant data on faculty performance and presented it in a large matrix. Each row is a faculty member, and each column is a particular category of professional accomplishments (awards, publications, etc.). Dr. Ram does not attempt to formalize the analysis of these data any further, and he still uses a 1 to 5 score. Evaluations are based on a consensus of the advisory committee, and this approach simply ensures they are looking at the same data. It seemed too simple.

----


Another approach exists that is not the most theoretically sound or even the most effective solution, but it is simple. If you have to make estimates for a list of similar items, some kind of weighted score is one way to go. If you are trying to estimate the relative “business opportunity” of, say, a list of real estate investments, you could identify a few major factors you consider important, evaluate these factors for each investment, and combine them somehow into an aggregate score. You might identify factors such as location desirability, cost, market growth for that type of real estate, liens, and so on. You might then weight each factor by multiplying it times some number and adding them all up to get a total value. While I used to categorically dismiss the value of weighted scores as something no better than astrology, subsequent research has convinced me that they may offer some benefit after all. Unfortunately, the methods that seem to have some benefits are not usually the ones businesses typically employ. According to the decision science researcher and author Jay Edward Russo, the efficacy of weighted scores “depends on what you are doing now. People usually have so far to go that even simple methods are a big improvement.” Indeed, even the simplest weighted scores might improve on human decision making—once certain errors introduced by the score itself are accounted for.

----


Although this simple method doesn’t directly address any of the cognitive biases we listed, the research by Dawes and Russo seems to indicate that this particular version of weighted scores might benefit decision making, if only a little. Just thinking about the problem this way seems to cause at least a slight uncertainty reduction and improvement in the decisions. However, for big and risky decisions, where the value of information is very high, we can and should get much more sophisticated than merely getting organized and using a weighted score.

----


Measuring Reading with Rasch A fascinating application of Rasch statistics is measuring the difficulty of reading text. Jack Stenner, PhD, is president and founder of MetaMetrics, Inc., and used Rasch models to develop the Lexile Framework for assessing reading and writing difficulty and proficiency. This framework integrates the measurement of tests, texts, and students, making universal comparisons in common languages possible in these areas for the first time. With a staff of 65, MetaMetrics has done more in this area than perhaps any other institution, public or private, including: ■ All major reading tests report measures in Lexiles. About 20 million U.S. students have reading ability measures in Lexiles. ■ The reading difficulty of over 200,000 books and tens of millions of magazine articles is measured in Lexiles. ■ The reading curricula of several textbook publishers are structured in Lexiles. ■ State and local education institutions are adopting Lexiles rapidly. A text of 100 Lexiles is first grade, while 1,700 Lexile text is found in Supreme Court decisions, scientific journals, and the like. MetaMetrics can predict that a 600-Lexile reader will have an average 75% comprehension of a 600-Lexile text. (This book is 1240 Lexiles.)

----


In 1961, a statistician named Georg Rasch developed a solution to this problem.10 He proposed a method for predicting the chance that a subject would correctly answer a true/false question based on (1) the percentage of other subjects in the population who answered that particular item correctly and (2) the percentage of other questions that the subject answered correctly. Even if test subjects were taking different tests, the performance on a test by a subject who never took it could be predicted with a computable error. First, Rasch computed the chance that a randomly selected person from the population of test subjects would answer a question correctly. This is simply the percentage of people who answered correctly from those who were given the opportunity to answer the question. This is called the “item difficulty.” Rasch then computed the log-odds for that probability. “Logodds” are simply the natural logarithm of the ratio of the chance of getting the answer right to the chance of getting it wrong. If the item difficulty was 65%, that meant that 35% of people got the answer right and 65% got it wrong. The ratio of getting it right to getting it wrong is .548, and the natural log of this is –0.619. If you like, you can write the Excel formula as: ln(P(A)/(1−P(A))= where P(A) is the chance of answering the item correctly. Rasch then did the same with the chance of that person getting any question right. Since this particular person was getting 82% of the answers right, the subject’s log-odds would be ln(.82 / .18), or 1.52. Finally, Rasch added these two log-odds together, giving −.619+1.52 = .9. To convert this back to a probability, you can write a formula in Excel as: 1/ 1 / exp(.9)+1= The calculation produces a value of 71%. This means that this subject has a 71% chance of answering that question correctly, given the difficulty of the question and the subject’s performance on other questions. Over a large number of questions and/or a large number of test subjects, we would find that when the subject/item chance of being correct is 70%, about 70% of those people got that item correct. Likewise for 90%, 80%, and so on. In a way, Rasch models are just another form of calibration of probabilities.

----


Amazingly, he also found that the formula, while simply based on expert judgments and no objective historical data, was better than the expert at making these judgments. For example, the formula based only on analysis of expert judgments would predict better than the expert in such problems as who would do well in graduate school or which tumor was malignant. This became known as the Lens Model.

----


judge inconsistency from the evaluations. The evaluations of experts usually vary even in identical situations. As discussed at the beginning of this chapter, human experts can be influenced by a variety of irrelevant factors yet still maintain the illusion of learning and expertise. The linear model of the expert’s evaluation, however, gives perfectly consistent valuations.

----


The seven-step process is simple enough. I’ve modified it somewhat from Brunswik’s original approach to account for some other methods (e.g., calibration of probabilities) we’ve learned about since Brunswik first developed this approach. (See Exhibit 12.3.) 1. Identify the experts who will participate. 2. If they will be assessing a probability or range, calibrate them. 3. Ask them to identify a list of factors relevant to the particular item they will be estimating (e.g., the duration of a software project affects the risk of failure or the income of a loan applicant affects the chance he will repay), but keep it down to 10 or fewer factors. 4. Generate a set of scenarios using a combination of values for each of the factors just identified—they can be based on real examples or purely hypothetical. Make 30 to 50 scenarios for each of the judges you are surveying. 5. Ask the experts to provide the relevant estimate for each scenario described. 6. Perform a regression analysis as described in Chapter 9. The independent “X” variables are those given to the judges for consideration. The dependent “Y” variable is the estimate the judge was asked to produce. 7. For each of the columns of data in your scenarios, there will be a coefficient displayed in the output table created by Excel. Pair up each

----


variable with its coefficient, multiply the coefficient by its data item, and then add up all these products for each of the coefficient/variable pairs—just as the section on multiregression analysis in Chapter 9 shows. This is the quantity you are trying to estimate. This process will produce a table with a series of weights for each of the variables in our model. Since the model has no inconsistency whatsoever, we know that at least some error has been reduced.


----


We can quickly estimate how much less uncertainty we have with this model by estimating the inconsistency of judges. We can estimate inconsistency by using some duplicate scenarios unknown to the judges. In other words, the seventh scenario in the list may be identical to the twenty-ninth scenario in the list. After looking at a couple of dozen scenarios, experts will forget that they already answered the same situation and often will give a slightly different answer. Thoughtful experts are fairly consistent in their evaluation of scenarios. Still, inconsistency accounts for 10% to 20% of the error in most expert estimates. This error is completely removed by the Lens Method.

----


Robyn Dawes, the proponent of simple, nonoptimized linear models, agrees that Brunswik’s method shows a significant improvement over unaided human judgment but argues that it might not be due to the “optimization” of weights from regression. In published research of four examples, Dawes showed that the Lens Model is only a slight improvement on what he has called “improper” models, where weights are not derived from regression but are all equal or, remarkably, randomly assigned.12 Dawes concluded that this is the case because, perhaps, the value of experts is simply in identifying factors and deciding whether each factor is “good” or “bad” (affecting whether they would be positive or negative weights), and that the exact magnitude of the weights do not have to be optimized with regression.


----


The regression models I use for business tend to have a few conditional rules, such as “The duration of a project is a differentiating factor only if it is more than a year—all projects less than a year are equally risky.” In that sense, the models are not strictly linear, but they get much better correlations than purely linear Lens Models.

----


But I also pointed out that in the cases I’ve assessed in the past decade, decomposition alone was sufficient to reduce uncertainty in only 25% of the high-information-value variables. In most cases where an effort was justified to reduce uncertainty, some empirical observation still was necessary. In contrast, the examples of measurements so many businesses seem to produce are only the decomposition types (i.e., the business case) without any attempt at empirical methods. Every variable was simply the initial estimate—either from a single expert or agreed to by “committee”—and was always a point value with no range to express any uncertainty about the variable. No survey, experiment, or even methods to improve subjective judgments were ever applied or even considered. The same people who enthusiastically submitted a business case as an example of measurement could not, no matter how much I pressed them, think of a single quantity in their CBA that was arrived at after some kind of real-world observation like a survey or experiment.

----


Scores are methods of attempting to express relative worth, preference, and so on, without employing a real unit of measure. Although scoring is fairly called one type of ordinal measurement system we discussed in Chapter 3, I’ve always considered an arbitrary score to be a sort of measurement wannabe. It introduces additional errors for six reasons. 1. Scoring methods tend to ignore problems of partition dependence mentioned in Chapter 11. Arbitrary choices about where to draw the line between different ordinal values—or even the number of ordinal values given—can have a very large effect on responses. 2. Scores are often used for situations where proper quantitative measures are feasible and would be much more enlightening (e.g., converting a perfectly good return on investment (ROI) to a score or computing risk as a score instead of treating it like an actuary or financial analyst would). 3. Researchers have shown that such ambiguous labels used by such scoring methods don’t help the decision maker at all and actually add an error of their own. One issue is that verbal labels or 5-point scales are interpreted very differently by those who assess risks and may come to “agreement” without realizing that they have very different conceptions about the underlying risk. This creates what one researcher refers to as “the illusion of communication.”14 4. Scores can be revealing if they are part of a survey of a large group (e.g., customer satisfaction surveys), but they are much less enlightening when individuals use them to “evaluate” options, strategies, investments, and the like. People are rarely surprised in some way by a set of scores they applied themselves. 5. Scores are merely ordinal, but many users add error when they treat these ordinal scores as a real quantity. As previously explained, a higher ordinal score means “more” but doesn’t say how much more. Multiplying and adding ordinal scores to other ordinal scores has consequences users are often not fully aware of. Therefore, the method is likely to have unintended consequences.15 6. Ordinal scales add a kind of extreme rounding error called “range compression.”16 When applied to risk analysis, the riskiest item in the “medium” risk category actually can be many times riskier than the least risky item in the same category. Many users of these methods tend to cluster their responses in a way that makes, for example, a 5-point scale behave more like a 2-point scale—effectively reducing the “resolution” of the method even further and lumping together risks that are, in fact, orders of magnitude different.

----


Once we adjust for certain known problems, human judgment is not a bad measurement instrument after all. If you have a large number of similar, recurring decisions, Rasch and Lens models can definitely reduce uncertainty by removing certain types of errors from human judgment. Even Dawes’s simple z-score seems to be a slight improvement on human judgment.

----


As a benchmark for comparison, we will use the objective linear model based purely on historical data. Unlike the other methods discussed in this chapter, historical models do not depend on human judgment in any way and, consequently, typically perform much better, as Meehl conclusively showed. Usually we’d prefer to use this sort of method, but in many cases where we need to quantify “immeasurables,” these detailed, objective, historical data are harder to come by. Hence the need for the other methods, such as Lens, Rasch, and so on. In Chapter 9, we discussed how to perform regression analysis to isolate and measure the effects of multiple variables. If we have lots of historical data on a particular recurring problem, complete documentation on each of the factors, and the factors are based on objective measures (not subjective scales), and we have recorded the actual results, we can create an objective linear model. While the Lens Model correlates input variables to expert estimates, the objective model correlates input variables to actual historical results. On each of the Lens Model studies mentioned in Exhibit 12.2, a regression model also was completed on historical data. The study shown about cancer patient life expectancy, for example, involved giving the doctors medical chart data on cancer patients and then building a Lens Model on their estimates for life expectancy. The study also kept track of actual life expectancy by continuing to track the patients. While the Lens Model of the physicians’ prognoses had just 2% less error than human judges, the objective model had fully 12% less error. For all the studies listed in Exhibit 12.2, the Lens Model had on average 5% less error than an unaided human estimator of a measurement while objective linear models had on average 30% less error than the human experts. Of course, even objective linear models are not the ultimate answer to improving on unaided human judgment. More elaborate decomposition of the problem, as we discussed in previous chapters, usually can reduce uncertainty even further. If we were to arrange these methods on a spectrum ranging from unaided and unorganized human intuition to the objective linear model, it would look something like Exhibit 12.5.

----


being efficient, he or she means that it is very hard to beat the market consistently. For any given stock at any given point in time, its price is just about as likely to move up as move down in the very short term. If this was not true, then market participants would bid up or sell off the stock accordingly until that “equilibrium” (if such a thing exists in the market) was achieved. This process of aggregating opinions is better at forecasting than almost any of the individual participants in the market are. Far better than an opinion poll, participants have an incentive not only to consider the questions carefully but even, especially when a lot of money is involved, to expend their own resources to get new information to analyze about the investment. People who place bids irrationally tend to run out of money faster and get out of the market. Irrational people also tend to be “random noise” that cancels out in a large market since irrational people are just as likely to overvalue a stock as undervalue it (although our “herd instinct” can magnify irrationality in markets). And because of the incentive for participation, news about the value of the company is quickly reflected in its stock price. This is exactly the type of mechanism

----


Phase 1 Workshops for decision definition, calibration training, and initial calibrated estimates Define the decision and identify relevant variables. Set up the “business case” for the decision, using these variables. Model the current state of uncertainty. Use calibrated 90% confidence intervals and probabilities in a Monte Carlo simulation. Calibration Training Phase 2 Starts with first information value calculation, then empirical measurements, and subsequent modifications to the model Compute the value of additional information. Use methods from Chapter 7 to determine what to measure and how much effort to spend on measuring it. No Is there a value to more information? Yes Measure where the information value is high. Reduce uncertainty using any of the methods mentioned in Chapters 8 through 13. Phase 3 Determining optimum decisions; findings and recommendations Optimize decision. Use the quantified risk/return boundary of the decision makers (Chapter 6 and 11) to determine which decision is preferred. (Return to Phase 1 for the follow-on decisions.) EXHIBIT 14.1 Summary of the AIE Process: The Universal Measurement Approach

----


Phase 0: Project Preparation Initial research. Interviews, secondary research, and prior reports are studied so the AIE analyst can get up to speed on the nature of the problem. Expert identification. Four or five experts who provide estimates is typical, but I’ve included as many as 20 (not recommended). Workshop planning. Four to six half-day workshops are scheduled with the identified experts. Phase 1: Decision Modeling Decision problem definition. In the first workshop, the experts identify what specific problem they are trying to analyze. For example, are they deciding whether to proceed with a particular investment, or is the

----


dilemma just about how to modify the investment? If the decision is an investment, project, commitment, or other initiative, we need to have a meeting with decision makers to develop an investment boundary for the organization. Decision model detail. By the second workshop, using an Excel spreadsheet, we list all of the factors that matter in the decision being analyzed and show how they add up. If it is a decision to approve a particular major project, we need to list all of the benefits and costs, add them into a cash flow, and compute an ROI (as in any simple business case). Initial calibrated estimates. In the remaining workshops, we calibrate the experts and fill in the values for the variables in the decision model. These values are not fixed points (unless values are known exactly). They are the calibrated expert estimates. All quantities are expressed as 90% confidence interval (CI) or other probability distributions. Phase 2: Optimal Measurements Value of information analysis (VIA). At this point, we run a VIA on every variable in the model. This tells us the information values and thresholds for every uncertain variable in the decision. A macro I wrote for Excel does this very quickly and accurately, but the methods discussed earlier in the book are a good estimate. Preliminary measurement method designs. From the VIA, we realize that most of the variables have sufficient certainty and require no further measurement beyond the initial calibrated estimate. Usually only a couple of variables have a high information value (and often they are somewhat of a surprise). Based on this information, we choose measurement methods that, while being significantly less than the Expected Value of Perfect Information (EVPI), should reduce uncertainty. The VIA also shows us the threshold of the measurement—that is, where it begins to make a difference to the decision. The measurement method is focused on reducing uncertainty about that relevant threshold. Measurements methods. Decomposition, random sampling, subjectiveBayesian, controlled experiments, Lens Models (and so on) or some combination thereof are all possible measurement methods used to reduce the uncertainty on the variables identified in the previous step. Updated decision model. We use the findings from the measurements to change the values in the decision model. Decomposed variables are shown explicitly in their decision model (e.g., an uncertain cost component may be decomposed into smaller components, and each of its 90% CIs is shown). Final value of information analysis. VIAs and measurements (the previous four steps) may go through more than one iteration. As long as the VIA shows a significant information value that is much greater than the cost of a measurement, measurement will continue. Usually, however, one or two iterations is all that is needed before the VIA indicates that no further measurements are economically justified. Phase 3: Decision Optimization and the Final Recommendation Completed risk/return analysis. A final Monte Carlo simulation shows the probabilities of possible outcomes. If the decision is about some major investment, project, commitment, or other initiative (it’s usually one of them), compare the risk and return to the investment boundary for the organization. Identified metrics procedures. There are often residual VIAs (variables with some information value that were not practical or economical to measure completely but would become obvious later on). Often these are variables about project progress or external factors about the business or economy. These are values that need to be tracked because knowing them can cause midcourse corrections. Procedures need to be put in place to measure them continually. Decision optimization. The real decision is rarely a simple “yes/no” approval process. Even if it were, there are multiple ways to improve a decision. Now that a detailed model of risk and return has been developed, risk mitigation strategies can be devised and the investment can be modified to increase return by using what-if analysis. Final report and presentation. The final report includes an overview of the decision model, VIA results, the measurements used, the position on the investment boundary, and any proposed ongoing metrics or analysis for the future, follow-on decisions. This seems like a lot to digest, but it is really just the culmination of everything covered in the book so far. Now let’s turn to a couple of examples in areas that many of the participants in my study presumed to be partly or entirely immeasurable.

